{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Bootcamp","text":"<p>This site contains the materials for Ka\u02bbimi's (very) opinionated software developer bootcamp. This bootcamp will cover a large variety of concepts neccessary to succeed as a data/analytics-oriented software developer. </p>"},{"location":"#structure-of-the-bootcamp","title":"Structure of the bootcamp:","text":"<ul> <li>Related content has been grouped into modules.<ul> <li>Each module spans at least one week (some are longer/shorter than others).<ul> <li>Each week will have two sessions, each discussing a primary concept.<ul> <li>Each session will be followed by recommended \"homework\" to apply the concepts discussed.</li> <li>Furthermore, recommended additional materials will be made available.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"advanced-python/","title":"Advanced Python","text":"<p>This module focuses on writing clean, professional-grade Python code and introduces the fundamental concepts of data engineering.</p>"},{"location":"advanced-python/#advanced-python-unit-testing","title":"Advanced Python &amp; Unit Testing","text":"<p>Session 1: From Functions to Classes This lesson builds on the students\u2019 understanding of functions by introducing object-oriented programming as a way to manage state and structure code. We\u2019ll explore pure and higher-order functions, then transition into defining classes, creating objects, and distinguishing between instance and class attributes. By the end, students will see why classes provide a natural way to model real-world entities and organize code beyond simple scripts.</p> <p>Session 2: Organizing and Designing Code Here we move from single classes into building larger, well-structured programs. Students will learn how to organize code into modules and packages, and deepen their understanding of object-oriented features like inheritance, composition, and encapsulation. We\u2019ll also introduce common design patterns, such as Strategy and Factory, to show how they make code more flexible and reusable. The goal is to give students the tools to design and scale their programs with clarity.</p> <p>Session 3: Writing Robust, Maintainable Code This focuses on professional practices that make Python code reliable and future-proof. We\u2019ll cover type hints and static analysis with mypy, how to use linters and formatters to enforce style and catch errors, and how to write clear, standardized docstrings. Finally, we\u2019ll introduce Pydantic as a practical way to validate and enforce structure in real-world data. By the end, students will know how to make their code both elegant and production-ready.</p>"},{"location":"advanced-python/#unit-testing-and-validation","title":"Unit Testing and Validation","text":"<p>Session 4: This session is all about writing high-quality, testable code. We'll introduce unit testing and practice writing tests for a Python application using the pytest framework. We'll emphasize the principles of test-driven development (TDD) to build features that are robust from the start.</p> <p>Session 5: We'll focus on data validation and error handling. Students will learn how to write tests that specifically challenge Pydantic models to ensure that incoming data is always correctly formatted. We'll also cover best practices for creating custom validation logic.</p>"},{"location":"advanced-python/1/","title":"Advanced Python Concepts - From Functions to Classes","text":"<p>We've built a strong foundation with our developer tools; now it's time to level up our Python skills. Today, we're transitioning from writing simple scripts to building more structured and organized applications. We'll explore advanced functional programming and then dive into Object-Oriented Programming (OOP), a powerful paradigm for managing complexity and modeling real-world entities in your code.</p>"},{"location":"advanced-python/1/#advanced-functional-programming","title":"Advanced Functional Programming","text":"<p>While we've used functions to organize code, Python also has powerful features that let us treat functions as \"first-class citizens.\" This means we can pass them as arguments, return them from other functions, and assign them to variables.</p>"},{"location":"advanced-python/1/#listset-comprehensions","title":"List/Set Comprehensions","text":"<p>List/set comprehensions provide a concise and elegant way to create lists or sets. They are often more readable and efficient than using a for loop. Sometimes this is simpler than defining a custom function/lambda and loops.</p> <pre><code># Using a for loop to create a list of squares\nsquares = []\nfor i in range(5):\n    squares.append(i * i)\n\n# The equivalent list comprehension\nsquares_comp = [i * i for i in range(5)]\n# Output: [0, 1, 4, 9, 16]\n\n# A set comprehension\neven_numbers = {i for i in range(10) if i % 2 == 0}\n# Output: {0, 2, 4, 6, 8}\n</code></pre>"},{"location":"advanced-python/1/#args-and-kwargs","title":"<code>*args</code> and <code>**kwargs</code>","text":"<p>These special syntax features allow functions to accept a variable number of arguments.</p> <p><code>*args</code> (non-keyword arguments): Gathers any number of positional arguments into a tuple.</p> <p><code>**kwargs</code> (keyword arguments): Gathers any number of keyword arguments into a dictionary.</p> <p>They are incredibly useful for writing flexible functions that can handle many different inputs.</p> <pre><code>def print_arguments(*args, **kwargs):\n    print(\"Positional arguments:\")\n    for arg in args:\n        print(f\" - {arg}\")\n    print(\"\\nKeyword arguments:\")\n    for key, value in kwargs.items():\n        print(f\" - {key}: {value}\")\n\nprint_arguments(\"hello\", 1, name=\"Alice\", age=30)\n# Output:\n# Positional arguments:\n#  - hello\n#  - 1\n#\n# Keyword arguments:\n#  - name: Alice\n#  - age: 30\n</code></pre>"},{"location":"advanced-python/1/#lambda-functions","title":"Lambda Functions","text":"<p>Lambda functions are small, anonymous functions defined with the <code>lambda</code> keyword. They are perfect for one-off operations where a full <code>def</code> statement would be overkill.</p> <pre><code># A standard function\ndef is_even(n):\n    return n % 2 == 0\n\n# The equivalent lambda function\nis_even_lambda = lambda n: n % 2 == 0\n\nprint(is_even(4))        # Output: True\nprint(is_even_lambda(5)) # Output: False\n</code></pre>"},{"location":"advanced-python/1/#higher-order-functions-and-functoolspartial","title":"Higher-Order Functions and <code>functools.partial</code>","text":"<p>A higher-order function is one that takes another function as an argument. The built-in <code>map()</code> and <code>filter()</code> functions are great examples. We can use them with lambda functions to write very concise, readable code.</p> <pre><code>numbers = [1, 2, 3, 4, 5, 6]\n\n# Use a lambda with filter() to get only even numbers\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\nprint(even_numbers) # Output: [2, 4, 6]\n</code></pre> <p>Sometimes, you need to use a function but want to \"pre-fill\" some of its arguments. That's what <code>functools.partial</code> is for. It lets you create a new, simpler version of a function with some of its arguments already provided.</p> <pre><code>from functools import partial\n\ndef power(base, exponent):\n    return base ** exponent\n\n# Create a new function that is a \"power of 2\" function\npower_of_two = partial(power, exponent=2)\n\nprint(power_of_two(4)) # Output: 16 (equivalent to 4 ** 2)\n</code></pre>"},{"location":"advanced-python/1/#function-decorators","title":"Function Decorators","text":"<p>A decorator is a special kind of higher-order function that adds new functionality to an existing function without changing its structure. They are commonly used for tasks like logging, timing, and access control. We apply a decorator using the <code>@</code> syntax.</p> <pre><code>graph TD\n    A[Original Function] --&gt; B{Decorator}\n    B -- wraps --&gt; C[New, Decorated Function]</code></pre> <p>Here is a simple example of a decorator that prints a message before and after a function runs.</p> <pre><code>def my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n</code></pre>"},{"location":"advanced-python/1/#object-oriented-programming-oop","title":"Object-Oriented Programming (OOP)","text":"<p>The two fundamental concepts in OOP are classes and objects.</p> <ul> <li>A class is a blueprint or a template for creating objects. Think of a class as a cookie cutter.</li> <li>An object is an instance of a class. It's the actual thing you create from the blueprint. Following our analogy, an object is the actual cookie.</li> </ul>"},{"location":"advanced-python/1/#defining-classes-methods-and-dunder-methods","title":"Defining Classes, Methods, and Dunder Methods","text":"<p>You define a class using the <code>class</code> keyword. A method is a function defined inside a class that describes an object's behavior.</p> <p>A dunder method (named for \"double underscore\") is a special method used by Python to implement core functionality. The most common is <code>__init__</code>, the initializer, which runs when you create a new object. Another useful one is <code>__str__</code>, which provides a user-friendly string representation of an object.</p> <pre><code>class User:\n    def __init__(self, name, email):\n        self.name = name\n        self.email = email\n        self.is_logged_in = False\n\n    def __str__(self):\n        return f\"{self.name} &lt;{self.email}&gt;\"\n\n    def login(self):\n        self.is_logged_in = True\n\nuser1 = User(\"Alice\", \"alice@example.com\")\nprint(user1) # This calls the __str__ method\n</code></pre> <p>Dun Dun Dun</p> <p>There are numerous powerful dunder methods that can super power your classes. See here for other examples.. For example, <code>__add__</code> can be used to define how you add two custom objects together.</p>"},{"location":"advanced-python/1/#inheritance-the-is-a-relationship","title":"Inheritance (The \"Is A\" Relationship)","text":"<p>Inheritance is a core concept that allows a class to inherit attributes and methods from another class. The inheriting class is called the child or subclass, and the class it inherits from is the parent or superclass. This is powerful for modeling relationships like \"is a.\" A <code>Student</code> is a <code>User</code>, so the <code>Student</code> class can inherit from the <code>User</code> class.</p> <pre><code>classDiagram\n    class User {\n        +name\n        +email\n        +login()\n    }\n    class Student {\n        +major\n        +enroll()\n    }\n    User &lt;|-- Student</code></pre> <pre><code>class Student(User): # Student inherits from User\n    def __init__(self, name, email, major):\n        super().__init__(name, email) # Call the parent's initializer\n        self.major = major\n\n    def enroll(self):\n        print(f\"{self.name} is now enrolled in {self.major}.\")\n\nstudent1 = Student(\"Bob\", \"bob@school.edu\", \"Computer Science\")\nstudent1.login() # This method was inherited from User!\nstudent1.enroll()\n</code></pre> <p>To <code>__init__</code> or not to <code>__init__</code></p> <p>When inheriting from a parent class which contains an <code>__init__</code> method, you don't have to supply an <code>__init__</code> in the child class. However, if you need to extend the initializer to include additional properties, you can use the <code>super().__init__(...)</code>.</p>"},{"location":"advanced-python/1/#composition-the-has-a-relationship","title":"Composition (The \"Has A\" Relationship)","text":"<p>Composition is when a class contains an instance of another class as an attribute. It models a \"has a\" relationship, and it is often preferred over inheritance because it's more flexible.</p> <ol> <li> <p>Let's define a simple <code>Engine</code> class in <code>core/components.py</code>. This class has the behavior of starting and stopping.</p> core/components.py<pre><code>class Engine:\n    def start(self):\n        return \"Engine started.\"\n\n    def stop(self):\n        return \"Engine stopped.\"\n\nif __name__ == \"__main__\":\n    engine = Engine()\n    print(\"Start your engines!\")\n    print(engine.start())\n</code></pre> </li> </ol> <p>What is <code>if __name__ == \"__main__\"</code>?</p> <p>This a convinent way to distinguish functionality between when you import the module and when you run it directly - and it is completely optional. It is incredibly useful for debugging purposes or documeting example usage. In other words, when you import from <code>core/components.py</code>, everything above the conditional is executed/defined, however the conditional itself only evaluates if you execute <code>core/components.py</code> directly. Now your file is accessible as both a stand-alone script as well as an importable module.</p> <ol> <li> <p>Now, in <code>transport/cars.py</code>, we'll create a <code>Car</code> class that has an <code>Engine</code>. We do this by creating an instance of the <code>Engine</code> class within the <code>Car</code>'s <code>__init__</code> method.</p> transport/cars.py<pre><code>from my_project.core.components import Engine\n\nclass Car:\n    def __init__(self):\n        # Composition: A Car object \"has an\" Engine object\n        self.engine = Engine()\n\n    def start(self):\n        return self.engine.start()\n\n    def stop(self):\n        return self.engine.stop()\n\nif __name__ == \"__main__\":\n    car = Car()\n    car.start()\n    car.stop()\n</code></pre> </li> </ol>"},{"location":"advanced-python/1/#inheritance-vs-composition","title":"Inheritance vs. Composition","text":"Concept Relationship Analogy When to Use Inheritance \"is a\" A Car is a Vehicle. When a class is a specialized version of another. Composition \"has a\" A Car has a Engine. When a class is composed of other objects. <p>Using both of these correctly is key to building robust and scalable applications.</p>"},{"location":"advanced-python/1/#static-and-class-methods","title":"Static and Class Methods","text":"<p>These are methods that belong to the class rather than an individual object.</p> <ul> <li>A static method doesn't require access to the instance (<code>self</code>) or the class (<code>cls</code>). It's just a regular function logically grouped within the class. We use the <code>@staticmethod</code> decorator.</li> <li>A class method receives the class itself as the first argument (<code>cls</code>). This is often used as a \"factory method\" to create an instance of the class from a different format. We use the <code>@classmethod</code> decorator.</li> </ul> <pre><code>class User:\n    @staticmethod\n    def is_valid_email(email):\n        return \"@\" in email\n\n    @classmethod\n    def create_from_tuple(cls, user_data):\n        name, email = user_data\n        return cls(name, email)\n\nuser_tuple = (\"Eve\", \"eve@example.com\")\nuser2 = User.create_from_tuple(user_tuple)\n</code></pre>"},{"location":"advanced-python/1/#recommended-exercises-homework","title":"Recommended Exercises &amp; Homework","text":"<p>For homework, you'll be tasked with applying these new functional and OOP concepts.</p>"},{"location":"advanced-python/1/#development-environment","title":"Development Environment","text":"<p>Recall, what you know about Git, Docker, and DevContainers to set yourself up for success. Set up a development environment which you can use for the rest of this advanced python section - and don't reinvent the wheel.</p>"},{"location":"advanced-python/1/#create-a-class-hierarchy-inheritance-is-a","title":"Create a Class Hierarchy (Inheritance - \"is a\")","text":"<ul> <li>Create a parent class named <code>Shape</code> with an <code>__init__</code> method that takes a <code>color</code> as an argument.</li> <li>Create two child classes, <code>Circle</code> and <code>Square</code>, that inherit from <code>Shape</code>.</li> <li>The <code>Circle</code> class should have an instance attribute for <code>radius</code>.</li> <li>The <code>Square</code> class should have an instance attribute for <code>side_length</code>.</li> <li>Both <code>Circle</code> and <code>Square</code> should have a method called <code>area()</code> that calculates and returns their respective area.</li> </ul> <p>More Advanced Abstractions</p> <p>There are simple ways to implement the above exercise. However, you can look into another advanced technique, <code>Abstract Base Class (abc)</code>, to do this better.</p>"},{"location":"advanced-python/1/#add-dunder-methods-and-inheritance","title":"Add Dunder Methods and Inheritance","text":"<ul> <li>To your <code>Shape</code>, <code>Circle</code>, and <code>Square</code> classes, add a <code>__str__</code> method that returns a user-friendly string representation of the object (e.g., <code>\"A red square with a side length of 5 and area 25.\"</code>).</li> <li>Implement a class method on the <code>Shape</code> class called <code>create_red_shape(cls)</code>. This method should return a new instance of the class (<code>cls</code>) with the color already set to <code>'red'</code>.</li> <li>Change the <code>area</code> calculation to a private method, which is automatically called sets a <code>self.area</code> property upon initialization.</li> <li>Implement <code>__add__</code> and <code>__sub__</code> methods, which defines you to add/subtract shapes together. As an example, make it so when you add two shapes, you return the sum of each area.</li> </ul>"},{"location":"advanced-python/1/#modeling-a-system-with-composition-has-a","title":"Modeling a System with Composition (\"has a\")","text":"<p>This exercise focuses on the \"has a\" relationship, where one object contains other objects.</p> <ul> <li> <p>Part A: Create Component Classes</p> <ul> <li>Create a class named <code>Engine</code> with an <code>__init__</code> method that accepts <code>fuel_type</code> (e.g., 'gasoline', 'electric').</li> <li>The <code>Engine</code> class should have a method <code>start()</code> that prints a message indicating the engine is starting with its fuel type (e.g., <code>\"Gasoline engine roaring to life!\"</code>).</li> <li>Create a class named <code>Wheel</code> with an <code>__init__</code> method that accepts <code>diameter</code> (an integer).</li> <li>The <code>Wheel</code> class should have a method <code>rotate()</code> that prints a message like <code>\"Wheel (20 inches) is rotating.\"</code>.</li> </ul> </li> <li> <p>Part B: Build the Main Class using Composition</p> <ul> <li>Create a class named <code>Car</code> with an <code>__init__</code> method that takes <code>model</code> (string), <code>fuel_type</code> (string), and <code>wheel_size</code> (integer) as arguments.</li> <li>Inside the <code>Car</code>'s <code>__init__</code> method, instantiate one <code>Engine</code> object and four <code>Wheel</code> objects using the provided arguments.</li> <li>The <code>Car</code> object should not inherit from <code>Engine</code> or <code>Wheel</code>.</li> </ul> </li> <li> <p>Part C: Delegate Behavior</p> <ul> <li>The Car class should have a method <code>drive()</code>. This method should:<ul> <li>Call the <code>start()</code> method of its internal <code>Engine</code> object.</li> <li>Iterate over its list of <code>Wheel</code> objects and call the <code>rotate()</code> method on each one.</li> <li>Print a final message: <code>\"The [model] is ready to go!\"</code></li> </ul> </li> </ul> </li> </ul> <p>Goal: This exercise demonstrates how the Car class composes its behavior by delegating tasks to its internal <code>Engine</code> and <code>Wheel</code> objects.</p>"},{"location":"advanced-python/1/#decorators-challenge","title":"Decorators Challenge","text":"<ul> <li>Create a new Python file and write a decorator called <code>@timer</code>. This decorator should print the executions time of the wrapped function.</li> <li>Apply this decorator to the <code>area()</code> method on your <code>Circle</code> and <code>Square</code> classes.</li> </ul>"},{"location":"advanced-python/1/#suggested-readings-resources","title":"Suggested Readings &amp; Resources","text":"<ul> <li>Python.org: <code>functools</code> module</li> <li>W3Schools: Python Decorators</li> <li>Real Python: Object-Oriented Programming (OOP) in Python 3</li> <li>Dunder Methods</li> <li><code>Abstract Base Class (abc)</code></li> </ul>"},{"location":"advanced-python/2/","title":"Advanced Python Concepts - Organizing and Designing Code","text":"<p>We learned that classes and objects provide a powerful way to model real-world concepts. Today, we're going to expand on that by learning how to structure entire applications. We'll go from single classes to well-organized programs, and we'll introduce design patterns that make our code more flexible and scalable.</p> <p>Functions vs Classes</p> <p>Although the following lesson is heavily focused on object-oriented design patterns, you don't have to consolidate all of your functions under classes! Sometimes, object oriented is actually a bad choice, both in terms of readability/maintainability but also performance. You can define functions in your module and import them when needed.</p>"},{"location":"advanced-python/2/#from-scripts-to-packages","title":"From Scripts to Packages","text":"<p>When a project is small, a single Python file works just fine. But as your codebase grows, it's essential to organize your code into logical, reusable units. These are called modules and packages.</p> <ul> <li>A module is simply a single Python file (<code>.py</code>).</li> <li>A package is a directory that contains multiple modules and a special (often empty) <code>__init__.py</code> file. This is how you group related functionality.</li> </ul> <p>Let's walk through an example. Imagine we have a project that involves different kinds of vehicles. A bad way to structure this would be to put all the classes in one file:</p> <pre><code>my_project/\n\u2514\u2500\u2500 vehicles.py  # A single, messy file with all classes\n</code></pre> <p>Instead, let's create a well-organized package for our project.</p> <ol> <li> <p>We'll start by creating a project directory with the following structure. The <code>__init__.py</code> files are what make Python recognize these folders as a package.</p> <p><code>__init__.py</code>: The Gatekeeper</p> <p>An <code>__init__.py</code> file serves two main purposes:</p> <ol> <li> <p>It tells Python that the directory should be treated as a package. Without this file, a directory with Python scripts is just a regular folder and can't be imported from.</p> </li> <li> <p>It can contain initialization code for the package. For example, you can use it to automatically import certain modules or define a public API for the package, so users can <code>import my_project.transport.Car</code> instead of the full path like <code>my_project.transport.cars.Car</code>.</p> </li> </ol> <pre><code>my_project/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 components.py  # For things like an Engine class\n\u251c\u2500\u2500 transport/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 cars.py        # For our Car class\n\u2514\u2500\u2500 main.py\n</code></pre> <p><code>ModuleNotFoundError</code>...Where am I?</p> <p>Note, a common annoyance of Python modules is making sure that Python/scripts know where to find your custom module(s). There are many ways to solve this issue, however since we are working in containers, in our build/compose we can set the <code>PYTHONPATH</code> environment variable to point to our module. For example <code>PYTHONPATH=\"${PYTHONPATH}:/path/to/my_project/\"</code></p> </li> <li> <p>Next, we'll write our <code>Engine</code> class in <code>core/components.py</code> and our <code>Car</code> class in <code>transport/cars.py</code>.</p> </li> <li> <p>Finally, we'll open <code>main.py</code> and demonstrate how easy it is to import and use classes from any module within the package:</p> main.py<pre><code>from my_project.core.components import Engine\nfrom my_project.transport.cars import Car\n\nengine = Engine()\ncar = Car(engine)\ncar.start()\n</code></pre> </li> </ol> <p>This structure makes our code more readable, reusable, and easy to navigate.</p>"},{"location":"advanced-python/2/#deepening-your-oop","title":"Deepening Your OOP","text":"<p>We've already learned about inheritance and composition, but there are other powerful OOP concepts which are crucial for building maintainable codebases.</p>"},{"location":"advanced-python/2/#encapsulation-hiding-complexity","title":"Encapsulation (Hiding Complexity)","text":"<p>Encapsulation is the principle of hiding an object's internal state and only exposing what is necessary. It protects your object's data from unintended changes. In Python, we use a naming convention to signal encapsulation:</p> <ul> <li><code>_variable_name</code> (single underscore): A convention that signals, \"This is an internal variable; please don't use it directly.\"</li> <li> <p><code>__variable_name</code> (double underscore): Python \"mangles\" the name, making it harder to access from outside the class.</p> </li> <li> <p>We'll update our <code>Engine</code> class to have a private attribute called <code>__rpm</code> to represent its speed.</p> </li> <li> <p>We'll create public methods (<code>get_rpm</code> and <code>set_rpm</code>) to control access to this data.</p> core/components.py<pre><code>class Engine:\n    def __init__(self):\n        self.__rpm = 0\n\n    def get_rpm(self):\n        return self.__rpm\n\n    def set_rpm(self, value):\n        if value &gt;= 0:\n            self.__rpm = value\n</code></pre> </li> </ul> <p>This ensures the <code>__rpm</code> attribute can only be modified through the <code>set_rpm</code> method, allowing us to enforce business logic (e.g., <code>rpm</code> can't be negative).</p>"},{"location":"advanced-python/2/#abstraction","title":"Abstraction","text":"<p>Abstraction is about simplifying complex reality by modeling classes based on the essential properties and behaviors of an object. You hide the internal details and only show what's necessary to the user. A great analogy is driving a car. You know how to start it and press the gas pedal, but you don't need to understand the internal combustion engine's intricate mechanics. The car's controls are the abstraction.</p> <p>In Python, you can achieve abstraction through abstract base classes (ABCs) using the <code>abc</code> module. An ABC can't be instantiated; it's meant to be inherited by other classes. It can also define abstract methods, which a subclass must implement. If a concrete subclass doesn't implement all the abstract methods, you'll get a <code>TypeError</code>.</p> <pre><code>from abc import ABC, abstractmethod\n\n# ABC cannot be instantiated\nclass Vehicle(ABC):\n    @abstractmethod\n    def start_engine(self):\n        pass\n\n    def drive(self):\n        print(\"Driving...\")\n\nclass Car(Vehicle):\n    def start_engine(self):\n        print(\"Car engine started.\")\n\n# This will raise a TypeError because `Motorcycle` doesn't implement `start_engine`\n# class Motorcycle(Vehicle):\n#     pass\n\ncar = Car()\ncar.start_engine()\ncar.drive()\n</code></pre>"},{"location":"advanced-python/2/#polymorphism","title":"Polymorphism","text":"<p>Polymorphism, which means \"many forms,\" is the ability of an object to take on many forms. In OOP, it refers to the ability of different classes to respond to the same method call in their own way. This allows you to write more flexible and reusable code.</p> <p>The most common form of polymorphism in Python is duck typing. The saying goes, \"If it looks like a duck, swims like a duck, and quacks like a duck, then it's a duck.\" In programming, this means if an object has the methods and properties you need, you can use it, regardless of its class. The two main ways to achieve polymorphism are method overriding and method overloading.</p> <ul> <li>Method Overriding: A subclass provides a specific implementation of a method that is already defined in its parent class. This is what you already did with the <code>__init__</code> method in your <code>Student</code> class example.</li> <li>Method Overloading: This involves defining multiple methods with the same name but with different parameters. Python doesn't support traditional method overloading like Java or C++. Instead, you can achieve similar functionality using optional arguments, default values, or variable-length arguments.</li> </ul> Polymorphism with Method Overriding<pre><code>class Dog:\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat:\n    def speak(self):\n        return \"Meow!\"\n\nclass Duck:\n    def speak(self):\n        return \"Quack!\"\n\ndef animal_sound(animal):\n    print(animal.speak())\n\nanimal_sound(Dog())\nanimal_sound(Cat())\nanimal_sound(Duck())\n</code></pre>"},{"location":"advanced-python/2/#introducing-design-patterns-strategies-for-common-problems","title":"Introducing Design Patterns: Strategies for Common Problems","text":"<p>A design pattern is a reusable solution to a common problem in software design. They aren't concrete code you copy and paste; they are templates you adapt to solve a specific problem. Knowing them gives you a common vocabulary to discuss code architecture.</p>"},{"location":"advanced-python/2/#the-factory-pattern","title":"The Factory Pattern","text":"<p>The Factory Pattern provides a centralized way to create objects without exposing the complex creation logic. It's especially useful when you need to create different types of objects based on some input.</p> <ol> <li>Let's expand on our <code>Car</code> and <code>Engine</code> classes. We'll create a <code>Truck</code> class that has a more powerful <code>TruckEngine</code>.</li> <li>Next, we'll create a central <code>VehicleFactory</code> class that can create different vehicles based on a type.</li> </ol> <pre><code>classDiagram\n    direction LR\n    class VehicleFactory {\n        +create_vehicle(vehicle_type)\n    }\n    class Vehicle\n    class Car\n    class Truck\n    Vehicle &lt;|-- Car\n    Vehicle &lt;|-- Truck\n    VehicleFactory ..&gt; Vehicle</code></pre> <p>Here's how we'd implement it:</p> transport/vehicles.py<pre><code>class Vehicle:\n    def __init__(self, engine):\n        self.engine = engine\n\n    def start(self):\n        return self.engine.start()\n</code></pre> transport/cars.py<pre><code>from my_project.core.components import Engine\nfrom my_project.transport.vehicle import Vehicle\n\nclass Car(Vehicle):\n    def __init__(self):\n        super().__init__(Engine())\n</code></pre> transport/trucks.py<pre><code>from my_project.core.components import HeavyDutyEngine\nfrom my_project.transport.vehicle import Vehicle\n\nclass Truck(Vehicle):\n    def __init__(self):\n        super().__init__(HeavyDutyEngine())\n</code></pre> factory.py<pre><code>from my_project.transport.cars import Car\nfrom my_project.transport.trucks import Truck\n\nclass VehicleFactory:\n    def create_vehicle(self, vehicle_type):\n        match vehicle_type:\n            case \"car\":\n                return Car()\n            case \"truck\":\n                return Truck()\n            case _:\n                raise ValueError(\"Unknown vehicle type\")\n</code></pre> <p>This design hides the logic of creating the correct <code>Car</code> or <code>Truck</code> from the rest of your application. You simply ask the factory for a \"car\" or a \"truck,\" and it handles the rest.</p>"},{"location":"advanced-python/2/#the-strategy-pattern","title":"The Strategy Pattern","text":"<p>The Strategy Pattern allows you to define a family of algorithms and make them interchangeable. This is useful when an object needs to perform an action but the specific implementation of that action can change.</p> <ol> <li> <p>Imagine our <code>Car</code> needs to warn the user about a low fuel level. We could have a <code>FuelWarning</code> method that uses a specific strategy.</p> </li> <li> <p>First, let's define a family of notification strategies:</p> core/components.py<pre><code>class NotificationStrategy:\n    def send_warning(self, message):\n        pass\n\nclass ConsoleStrategy(NotificationStrategy):\n    def send_warning(self, message):\n        print(f\"[CONSOLE WARNING]: {message}\")\n\nclass EmailStrategy(NotificationStrategy):\n    def send_warning(self, message):\n        # In a real app, this would send an email\n        print(f\"[EMAIL WARNING]: {message}\")\n</code></pre> </li> <li> <p>Now, our <code>Car</code> class can use composition to hold a reference to a <code>NotificationStrategy</code> object.</p> <p>transport/cars.py<pre><code>from my_project.core.components import ConsoleStrategy, Engine\nfrom my_project.transport.vehicle import Vehicle\n\nclass Car(Vehicle):\n    def __init__(self, notification_strategy: NotificationStrategy):  # (1)!\n        super().__init__(Engine())\n        self.notification_strategy = notification_strategy\n\n    def check_fuel(self, fuel_level):\n        if fuel_level &lt; 5:\n            message = \"Fuel level is low!\"\n            self.notification_strategy.send_warning(message)\n</code></pre></p> <ol> <li>The <code>notification_strategy: NotificationStrategy</code> is called a type hint, we will expand on this in the next lesson.</li> </ol> </li> </ol> <p>This design allows us to easily switch between sending console warnings and emails by simply passing a different strategy object when creating the <code>Car</code>.</p>"},{"location":"advanced-python/2/#updated-exercises-homework","title":"Updated Exercises &amp; Homework","text":"<p>Your homework is to apply these code organization and design principles to build a single, cohesive application. Follow these steps in the order they are presented.</p> <ol> <li> <p>Project Setup:</p> <ul> <li>Create a new project folder called <code>my_project</code>.</li> <li>Inside it, set up the following package structure. Make sure to add <code>__init__.py</code> files to each folder to define them as Python packages.</li> </ul> <pre><code>my_project/\n\u251c\u2500\u2500 transport/\n\u2502   \u2514\u2500\u2500 vehicles/\n\u2514\u2500\u2500 notifications/\n</code></pre> </li> <li> <p>Abstraction with Abstract Base Classes:</p> <ul> <li>In the <code>transport</code> subpackage, create a file named <code>base.py</code>.</li> <li>Using the <code>abc</code> module, define an abstract base class called <code>Vehicle</code>.</li> <li>This class should have a private attribute <code>__fuel_level</code> initialized to <code>100</code> and a concrete method <code>get_fuel_level()</code> that returns the fuel level.</li> <li>Add an abstract method <code>refuel()</code> that takes <code>amount</code> as an argument. The <code>refuel</code> method should be responsible for updating the <code>__fuel_level</code>, but its specific implementation will be handled by subclasses.</li> <li>This step establishes the core contract that all vehicles must follow.</li> </ul> </li> <li> <p>Polymorphism with Method Overriding:</p> <ul> <li>Inside <code>transport/vehicles/</code>, create two files: <code>cars.py</code> and <code>motorcycles.py</code>.</li> <li>In <code>cars.py</code>, create a class <code>Car</code> that inherits from the abstract <code>Vehicle</code> class (from <code>transport.base</code>).</li> <li>In <code>motorcycles.py</code>, create a class <code>Motorcycle</code> that also inherits from <code>Vehicle</code>.</li> <li>Each of these concrete classes must provide its own implementation of the abstract <code>refuel()</code> method, which they inherited. For example, the <code>Car.refuel()</code> method could print \"Car is refueling...\" and update the fuel level, while the <code>Motorcycle.refuel()</code> method prints \"Motorcycle is refueling...\".</li> <li>This demonstrates polymorphism by showing different objects responding to the same method call (<code>refuel()</code>) in their own unique way.</li> </ul> </li> <li> <p>Implementing the Strategy Pattern:</p> <ul> <li>Inside the <code>notifications</code> package, create a file named <code>base.py</code>.</li> <li>In this file, define an abstract base class <code>FuelWarningStrategy</code> with an abstract method <code>send_warning(message)</code>.</li> <li>Next, create a file <code>console_strategy.py</code> in the same <code>notifications</code> package. In this file, create a concrete class <code>ConsoleWarningStrategy</code> that inherits from <code>FuelWarningStrategy</code> and implements the <code>send_warning()</code> method by printing the message to the console.</li> <li>Create another file <code>email_strategy.py</code> and implement a concrete class <code>EmailWarningStrategy</code> that prints a simulated email message.</li> <li>This prepares the different \"strategies\" that our vehicle objects will use.</li> </ul> </li> <li> <p>Putting It All Together with Composition:</p> <ul> <li>Update your <code>Vehicle</code> abstract class in <code>transport/base.py</code>.</li> <li>Add a concrete method called <code>check_fuel()</code> to this class. This method should accept a <code>FuelWarningStrategy</code> object as an argument.</li> <li>Inside <code>check_fuel()</code>, add logic to check if the vehicle's fuel level is below a certain threshold (e.g., less than 50).</li> <li>If the fuel is low, call the <code>send_warning()</code> method on the provided strategy object with a message. This demonstrates composition\u2014a <code>Vehicle</code> object \"has a\" <code>FuelWarningStrategy</code> object.</li> </ul> </li> <li> <p>Creating the Vehicle Factory:</p> <ul> <li>Inside <code>transport/vehicles/</code>, create a file <code>vehicle_factory.py</code>.</li> <li>Create a <code>VehicleFactory</code> class that has a single method, <code>create_vehicle(vehicle_type)</code>.</li> <li>This method should return a new instance of a <code>Car</code> or <code>Motorcycle</code> based on the <code>vehicle_type</code> string.</li> </ul> </li> <li> <p>Final Execution:</p> <ul> <li>In the root <code>my_project</code> directory, create a <code>main.py</code> file.</li> <li>Import your <code>VehicleFactory</code>, your concrete vehicle classes, and your warning strategies.</li> <li>Use the <code>VehicleFactory</code> to create a <code>Car</code> and a <code>Motorcycle</code>.</li> <li>Call the <code>refuel()</code> method on one of the vehicles to demonstrate the polymorphic behavior.</li> <li>Next, call the <code>check_fuel()</code> method on both vehicles, but pass a different strategy object to each one (e.g., <code>Car.check_fuel(ConsoleWarningStrategy())</code> and <code>Motorcycle.check_fuel(EmailWarningStrategy())</code>). This will show how your design allows for flexible and interchangeable behavior.</li> </ul> </li> </ol> <p>Final Directory Structure</p> <p>Here is the final, complete directory structure for your project. This layout is what you will have created after completing all the exercises. The use of <code>__init__.py</code> files is crucial as they tell Python that these directories are packages and can be imported from.</p> <pre><code>my_project/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 transport/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2514\u2500\u2500 vehicles/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 cars.py\n\u2502       \u251c\u2500\u2500 motorcycles.py\n\u2502       \u2514\u2500\u2500 vehicle_factory.py\n\u2514\u2500\u2500 notifications/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 base.py\n    \u251c\u2500\u2500 console_strategy.py\n    \u2514\u2500\u2500 email_strategy.py\n</code></pre> <p>This structure effectively separates the different components of your application:</p> <ul> <li><code>main.py</code>: The entry point of your program.</li> <li><code>transport/</code>: Holds all classes related to vehicles.</li> <li><code>transport/base.py</code>: Contains the abstract <code>Vehicle</code> class, which acts as the blueprint for all vehicles.</li> <li><code>transport/vehicles/</code>: Contains the concrete vehicle implementations (<code>Car</code> and <code>Motorcycle</code>) and the <code>VehicleFactory</code>.</li> <li><code>notifications/</code>: Contains all the different warning strategies.</li> <li><code>notifications/base.py</code>: Holds the abstract <code>FuelWarningStrategy</code> class.</li> <li><code>notifications/console_strategy.py</code> and <code>email_strategy.py</code>: The concrete implementations of the warning strategies.</li> </ul>"},{"location":"advanced-python/2/#suggested-readings-resources","title":"Suggested Readings &amp; Resources","text":"<ul> <li>Real Python: Python Packages</li> <li>Refactoring Guru: Design Patterns - A fantastic resource for learning about design patterns.</li> <li>Python.org: Modules</li> <li>Python ABCs</li> <li>More on Duck Typing</li> </ul>"},{"location":"advanced-python/3/","title":"Week 3, Session 3: Writing Robust, Maintainable Code","text":"<p>In our previous sessions, you learned to write structured, well-organized code using object-oriented principles. Today, we're focusing on the practices that distinguish good code from great code: robustness, readability, and maintainability. We'll integrate these concepts into a continuous process, showing you how to transform \"code smells\" into professional, production-ready code.</p>"},{"location":"advanced-python/3/#the-starting-point-code-smells","title":"The Starting Point: Code Smells","text":"<p>We'll use these two examples to guide our lesson. These patterns are common in rapid prototyping but lead to maintenance headaches:</p> <ul> <li> <p>Code Smell 1: Ambiguous Logic and Return Types</p> <p><pre><code>def do_it(x, y):  # (1)!\n    z = x + y if x else y * 2\n    if z &gt; 100:\n        return {\"a\": z}\n    return z  # (2)!\n</code></pre></p> <ol> <li>What are the inputs (<code>x</code>, <code>y</code>)? An <code>int</code>? A <code>str</code>?</li> <li>What are the outputs? It can return an <code>int</code>, or a <code>dict</code>, leading to confusion for anyone calling this function. </li> </ol> </li> <li> <p>Code Smell 2: Unstructured Data and Fragile Access</p> <p><pre><code>def process(data):\n    if (data[\"age\"] &gt; 18 and data[\"status\"] == \"active\") or (data[\"access\"][\"level\"] == \"ADMIN\"):  # (1)!\n        return {\"ok\": True, \"msg\": \"allowed\"}\n    return {\"ok\": False}\n</code></pre></p> <ol> <li>This code assumes the input <code>data</code> (a dictionary) has the keys <code>age</code> (which is an integer) and <code>status</code> (which is a string). If any key is missing or the type is wrong, the code will crash at runtime.</li> </ol> </li> </ul>"},{"location":"advanced-python/3/#tool-1-type-hints-and-static-analysis-fixing-ambiguity","title":"Tool 1: Type Hints and Static Analysis (Fixing Ambiguity)","text":"<p>The first step in professional development is clarifying intent. Type hints tell developers and machines what types of data are expected, solving the ambiguity in Code Smell 1.</p> <p>Applying Type Hints</p> <ol> <li> <p>We start by clarifying the inputs to <code>do_it</code> are numbers, but we still have an ambiguous return type (<code>Union</code>).</p> </li> <li> <p>We introduce Enums and Literals to enforce highly specific input values, solving common logic bugs before they happen.</p> <pre><code>from typing import Literal, Union\nfrom enum import StrEnum\n\n# Use StrEnum for status to eliminate magic strings\nclass UserStatus(StrEnum):\n    ACTIVE = \"active\"\n    INACTIVE = \"inactive\"\n    SUSPENDED = \"suspended\"\n\n# Example: Use Literal to constrain the column name to specific strings\ndef sort_data(column: Literal[\"date\", \"name\"], direction: Literal[\"asc\", \"desc\"]) -&gt; None:\n    pass\n\n# Applying to Code Smell 1 (conceptually):\n# This clarifies the input must be integers, but the output is complex.\n# The static checker (mypy) would immediately flag the inconsistent return types.\ndef do_it_typed(x: int, y: int) -&gt; Union[int, Dict[str, int]]:\n    # ... logic\n    pass\n</code></pre> </li> </ol> <p>Static Analysis: After adding type hints, we run <code>mypy</code> to perform static analysis. <code>mypy</code> is the static type checker that runs before execution and flags the ambiguity in <code>do_it_typed</code> (returning <code>int</code> sometimes and <code>dict</code> other times). This forces the developer to refactor the logic for clarity.</p>"},{"location":"advanced-python/3/#tool-2-linting-formatting-and-docstrings-fixing-readability","title":"Tool 2: Linting, Formatting, and Docstrings (Fixing Readability)","text":"<p>Once the code is logically correct, we make it beautiful and easy to read.</p>"},{"location":"advanced-python/3/#linting-and-formatting-with-ruff","title":"Linting and Formatting with Ruff","text":"<p>Ruff is an extremely fast tool that combines both linting (checking for programmatic errors) and formatting (enforcing style). It ensures all code is consistently spaced and follows best practices.</p> <p>Running Ruff</p> <ol> <li>We run <code>ruff format .</code> on the entire project. This instantly fixes indentation, line length, and spacing issues.</li> <li>We run <code>ruff check .</code> to enforce things like Python's naming conventions and complexity rules.</li> </ol> <p>This process is typically automated using a pre-commit hook, ensuring that bad style or format never even makes it into the version control history.</p>"},{"location":"advanced-python/3/#docstrings","title":"Docstrings","text":"<p>A docstring explains what the code does, why it does it, and what inputs/outputs are involved.</p> <p>Standardized Docstrings</p> <ol> <li> <p>We will use the VS Code extension <code>njpwerner.autodocstring</code> to quickly generate a template.</p> </li> <li> <p>We'll adopt the Google Style because it's highly readable and uses clear <code>Args:</code>, <code>Returns:</code>, and <code>Raises:</code> sections.</p> <pre><code>def calculate_compound_interest(principal: float, rate: float, time: int) -&gt; float:\n    \"\"\"Calculates the compound interest over a period of time. (Google Style)\n\n    Args:\n        principal (float): The initial amount of money (P).\n        rate (float): The annual interest rate (r), as a decimal.\n        time (int): The number of years (t).\n\n    Returns:\n        float: The total amount accrued (A).\n    \"\"\"\n    return principal * (1 + rate) ** time\n</code></pre> </li> </ol>"},{"location":"advanced-python/3/#tool-3-pydantic-fixing-fragile-data-structures","title":"Tool 3: Pydantic (Fixing Fragile Data Structures)","text":"<p>Pydantic is the solution to Code Smell 2 (Unstructured Data). It forces dictionary-like data to conform to an object-oriented structure, eliminating runtime errors caused by missing keys or wrong types.</p>"},{"location":"advanced-python/3/#building-robust-data-models","title":"Building Robust Data Models","text":"<p>We will refactor the <code>process(data)</code> function by defining its input structure as a Pydantic <code>BaseModel</code>.</p> <p>Basic Model Definition:We define the expected input using type hints. Pydantic validates and coerces the data upon instantiation.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import List, Optional\n\n# The model defines the structure for the input data\nclass User(BaseModel):\n    age: int\n    status: UserStatus # Using our custom Enum\n\n# Refactored function: Input is now a robust User object, not a fragile dict.\ndef process_user(user: User):\n    if user.age &gt; 18 and user.status == UserStatus.ACTIVE:\n        return {\"ok\": True, \"msg\": \"allowed\"}\n    return {\"ok\": False}\n</code></pre>"},{"location":"advanced-python/3/#complex-models-inheritance-composition","title":"Complex Models (Inheritance &amp; Composition):","text":"<p>We use OOP principles to define complex data structures, which are then used as type hints.</p> <ul> <li>Inheritance: We extend a base model to add specific fields.</li> <li>Composition: A model \"has a\" list of other models.</li> </ul> <p><pre><code>from datetime import date\nfrom pydantic import BaseModel, Field\n\n# Inheritance: TeamMember is a special type of User\nclass TeamMember(User):\n    team_id: str\n    is_lead: bool = False # Field with a default value\n\n# Composition: Project model \"has a\" list of TeamMember objects\nclass Project(BaseModel):\n    name: str = Field()\n    members: List[TeamMember]\n    date: Optional[date] = None  # Optional field\n    budget: Optional[float] = Field(default=None, ge=0, le=999_999_999.0) \n\n# When we create a Project, Pydantic validates ALL nested models automatically!\nproject_data = {\n    \"name\": \"Apollo\",\n    \"members\": [\n        {\"age\": 35, \"status\": \"active\", \"team_id\": \"T101\", \"name\": \"Alice\"},\n        # The following would fail validation: missing 'age' will raise an error\n        # {\"status\": \"active\", \"team_id\": \"T102\", \"name\": \"Bob\"}\n    ],\n    \"date\": date.today()\n    \"budget\": 1_000_000.0\n}\n\n# Try to unpack the data into a Project\ntry:\n    project = Project(**project_data)  # (1)!\n\nexcept ValidationError as e:\n    print(f\"Validation Error: {e}\")\n</code></pre></p> <ol> <li>Alternatively, we have other options.<ul> <li>We could use the <code>.parse_obj()</code> <code>classmethod</code>, like <code>Project.parse_obj(project_data)</code>.</li> <li>Fill out the model directly:     <pre><code>project = Project(\n    name=\"Apollo\",\n    members=[\n        TeamMember(age=35, status=\"active\", team_id=\"T101\", name=\"Alice\"),\n        ...\n    ],\n    ...\n)\n</code></pre> </li> </ul> </li> </ol>"},{"location":"advanced-python/3/#the-good-code-result","title":"The \"Good Code\" Result","text":"<p>By applying these practices, the initial \"code smells\" are transformed into clear, reliable, and production-ready code:</p> Old Code Smell Applied Tools New, Robust Code Code Smell 1 (Ambiguity) Type Hints, Ruff, Docstrings The types are clear, and the logic is broken into readable, single-purpose functions (which <code>ruff</code> encourages). The return type is always <code>int</code> for consistency. Code Smell 2 (Fragile Data) Pydantic, Type Hints The function input is a Pydantic object, eliminating runtime key errors. Logic is clean. Code Smell 1<pre><code>def do_it(x: int, y: int) -&gt; int | dict[str, int]:\n    \"\"\"\n    Computes a value based on x and y.\n    If x is truthy, z = x + y; otherwise, z = y * 2.\n    Returns a dictionary {\"a\": z} if z &gt; 100, else returns z.\n\n    Args:\n        x (int): First integer input.\n        y (int): Second integer input.\n\n    Returns:\n        int | dict[str, int]: Result based on computation.\n    \"\"\"\n    z = x + y if x else y * 2\n    if z &gt; 100:\n        return {\"a\": z}\n    return z\n</code></pre> <p>Dictionaries or Pydantic?</p> <p>As a personal preference, I usually default to Pydantic objects rather than working with dictionary types, however you don't have to chose only one. Sometimes you want the speed/comfort of the standard <code>dict()</code> or <code>set()</code>, but sometimes there is a benefit to have something with more defined structure.</p> Code Smell 2<pre><code>from enum import StrEnum\nfrom pydantic import BaseModel\n\nclass AccessLevelsEnum(StrEnum):\n    ADMIN = \"ADMIN\"\n    MEMBER = \"MEMBER\"\n\nclass UserStatusEnum(StrEnum):\n    ACTIVE = \"ACTIVE\"\n    INACTIVE = \"INACTIVE\"\n    SUSPENDED = \"SUSPENDED\"\n\nclass Permissions(BaseModel):\n    level: AccessLevelsEnum = AccessLevelsEnum.MEMBER\n    extra: str | None = None  # Some other random placeholder field\n\nclass InputData(BaseModel):\n    age: int\n    status: UserStatusEnum = UserStatusEnum.ACTIVE\n    permissions: Permissions\n\nclass OutputData(BaseModel):\n    ok: bool\n    msg: str | None = None\n\ndef process(data: InputData) -&gt; OutputData:\n    \"\"\"\n    Checks if the user is allowed based on age and status.\n\n    Args:\n        data (InputData): Input data containing age, status, and extra permissions information.\n\n    Returns:\n        OutputData: Result indicating if allowed, with a message if allowed.\n    \"\"\"\n    if (data.age &gt; 18 and data.status == UserStatusEnum.ACTIVE) or (data.permissions.level == AccessLevelsEnum.ADMIN):\n        return OutputData(ok=True, msg=\"allowed\")\n    return OutputData(ok=False)\n</code></pre> <p>Class Structure</p> <pre><code>classDiagram\n    class AccessLevelsEnum {\n        ADMIN\n        MEMBER\n    }\n    class UserStatusEnum {\n        ACTIVE\n        INACTIVE\n        SUSPENDED\n    }\n    class Permissions {\n        level: AccessLevelsEnum\n        extra: str | None\n    }\n    class InputData {\n        age: int\n        status: UserStatusEnum\n        permissions: Permissions\n    }\n    class OutputData {\n        ok: bool\n        msg: str | None\n    }\n    Permissions --&gt; AccessLevelsEnum\n    InputData --&gt; UserStatusEnum\n    InputData --&gt; Permissions</code></pre>"},{"location":"advanced-python/3/#recommended-exercises-homework","title":"Recommended Exercises &amp; Homework","text":"<p>This week's homework is a comprehensive exercise that asks you to take a \"bad code\" example and apply all the professional practices we covered today.</p> <p>The Starting Code (Configuration Loader):</p> <p>Imagine you are given this script that loads a complex configuration dictionary for a hypothetical application:</p> app_config_loader.py<pre><code>def load_config(settings):\n    # Very fragile access, no type checking, poor structure\n    if settings.get(\"database\").get(\"port\") == 5432:\n        conn_string = f\"postgresql://{settings['database']['user']}@{settings['database']['host']}\"\n        print(\"Using PostgreSQL default port.\")\n    else:\n        conn_string = \"invalid\"\n\n    if settings.get(\"environment\") == \"dev\":\n        is_debug = True\n    else:\n        is_debug = False\n\n    return {\"connection\": conn_string, \"debug\": is_debug}\n</code></pre> <p>Your Tasks (Refactor and Validate):</p> <ol> <li>Code Organization &amp; Readability:<ul> <li>Install ruff into your choice of development environment (preferably devcontainer).</li> <li>Run <code>ruff format</code> and <code>ruff check</code> on the script. Address and fix any errors it reports.</li> <li>Write clear, standardized docstrings (e.g., Google Style) for all functions and methods.</li> </ul> </li> <li>Type Constraint Implementation:<ul> <li>Create a custom <code>StrEnum</code> for the environment setting (e.g., <code>Environment.DEV</code>, <code>Environment.PROD</code>).</li> <li>Create a <code>Literal</code> type to constrain the allowed database type (e.g., <code>\"postgresql\"</code>, <code>\"mysql\"</code>).</li> </ul> </li> <li>Pydantic Validation (Complex Structures):<ul> <li>Define a <code>BaseModel</code> called <code>DBConfig</code> that uses the appropriate type hints for host, port, user, type (using your <code>Literal</code>), and password.</li> <li>Define a final model, <code>AppConfig</code>, that uses composition to include the <code>DBConfig</code> model and includes its own field for <code>environment</code> (using your custom <code>StrEnum</code>).</li> <li>Modify <code>load_config</code> to accept and return your <code>AppConfig</code> Pydantic model, ensuring that the function will now robustly validate any input data automatically.</li> </ul> </li> </ol> <p>Challenge: Auto-computed Fields</p> <p>Try to include a new auto calculated field to <code>AppConfig</code> called <code>conn_string</code>, which is the combination of all the fields needed to reconstruct the string <code>f\"postgresql://{settings['database']['user']}@{settings['database']['host']}\"</code>.</p> <p>This exercise will give you hands-on experience in making a codebase robust and production-ready.</p>"},{"location":"advanced-python/3/#suggested-readings-resources","title":"Suggested Readings &amp; Resources","text":"<ul> <li>Ruff Documentation: Getting Started with Ruff - Learn how to set up and configure this powerful linter/formatter.</li> <li>Pydantic Documentation: Pydantic v2 Documentation - Explore the \"Defining models\" and \"Validation\" sections.</li> <li>Real Python: Python Docstrings Guide - Excellent resource showing different docstring styles.</li> <li>Python <code>typing</code>: PEP 586 (Literal Types) and PEP 647 (Type Guards) - Deepen your understanding of specific type hints.</li> </ul>"},{"location":"dev-env/","title":"Foundational Tools &amp; Environment","text":"<p>This module is designed to get students set up with the essential tools for collaborative and reproducible software development. They will master Git and GitHub, learn to use Visual Studio Code, and then dive into the world of containers with Docker, Docker Compose, and DevContainers.</p>"},{"location":"dev-env/#git-github-and-visual-studio-code","title":"Git, GitHub, and Visual Studio Code","text":"<p>Session 1: Git &amp; GitHub Essentials \ud83d\udcbb This session covers the fundamentals of version control. Students will learn core Git commands like init, add, commit, and status. We'll also cover the collaborative aspects of GitHub, including push, pull, and the concept of a pull request.</p> <p>Session 2: Visual Studio Code Mastery \u270d\ufe0f Students will be introduced to Visual Studio Code (VS Code), our primary development environment for the bootcamp. We'll explore its user interface, essential shortcuts, and key extensions. The goal is for students to become comfortable and efficient with the editor.</p>"},{"location":"dev-env/#docker-docker-compose-and-devcontainers","title":"Docker, Docker Compose, and DevContainers","text":"<p>Session 3: Docker Fundamentals \ud83d\udc33 This session introduces Docker, explaining the difference between containers and virtual machines. Students will write their first Dockerfile to containerize a simple Python application.</p> <p>Session 4: Container Orchestration \u2699\ufe0f Building on the first session, we'll introduce Docker Compose to manage multi-service applications (e.g., a web app and a database). </p> <p>Session 5: Reproducible Environments The Docker lessons will culminate in setting up DevContainers for our course, ensuring every user can build identical, reproducible development environments from this point on.</p>"},{"location":"dev-env/1/","title":"Session 1: Git &amp; GitHub Essentials \ud83d\udcbb","text":"<p>Welcome to the first session of our software development bootcamp! Today, we're diving into the world of Git and GitHub, essential tools for any modern developer. We'll be focusing on using the GitHub Desktop application, which provides a user-friendly graphical interface for managing your code.</p>"},{"location":"dev-env/1/#introduction-to-version-control","title":"Introduction to Version Control","text":"<p>Imagine you're writing a really important document. You save it, then make some changes, and save it again. What if you want to go back to an earlier version? Or what if you're working with a team, and everyone is making changes at the same time? This is where Version Control Systems (VCS) come in!</p> <p>Git is the most popular distributed version control system. It helps you:</p> <ul> <li>Track changes to your code over time.</li> <li>Revert to previous versions if something goes wrong.</li> <li>Collaborate with other developers without overwriting each other's work.</li> </ul> <p>A repository (often shortened to \"repo\") is simply a project folder that Git is tracking.</p>"},{"location":"dev-env/1/#getting-started-with-github-desktop","title":"Getting Started with GitHub Desktop","text":"<p>Other Options</p> <p>There are numerous other options for working with git-like tools, besides GitHub Desktop (such as using the git CLI directly). However, I find that GitHub Desktop covers all we need and is far more beginner friendly.</p> <p>Before we dive into Git concepts, let's get you set up with the tool we'll be using: GitHub Desktop.</p> <p>Installation and Sign-In:</p> <ol> <li>Download GitHub Desktop: If you haven't already, go to GitHub Desktop and download the application for your operating system.</li> <li>Install: Follow the on-screen instructions to install GitHub Desktop.</li> <li>Sign In: When you open GitHub Desktop for the first time, you'll be prompted to sign in with your GitHub account. If you don't have one, you'll need to create one on github.com first. This links your local GitHub Desktop application to your online GitHub profile.</li> </ol>"},{"location":"dev-env/1/#basic-git-concepts-illustrated-with-github-desktop-in-mind","title":"Basic Git Concepts (Illustrated with GitHub Desktop in Mind)","text":"<p>Let's look at the core ideas behind Git that GitHub Desktop helps you manage.</p>"},{"location":"dev-env/1/#commits-snapshots-of-your-work","title":"Commits: Snapshots of Your Work","text":"<p>Think of a commit as a snapshot of your project at a specific point in time. Every time you save a set of related changes, you create a commit. Each commit has a unique ID, a message describing the changes, and information about who made the commit and when.</p> <pre><code>graph LR\n    A[Start Project] --&gt; B{Add File 1};\n    B --&gt; C(Commit 'Initial commit');\n    C --&gt; D{Modify File 1};\n    D --&gt; E(Commit 'Update File 1 content');\n    E --&gt; F{Add File 2};\n    F --&gt; G(Commit 'Add new feature');</code></pre> <p>In GitHub Desktop, the \"Changes\" tab shows you what files you've modified. Once you've selected the changes you want to save, you'll write a commit message and click \"Commit to [Branch Name]\".</p> <p>Making good commit messages</p> <p>At the bare minimum, a commit message must descriptive enough to describe the change. However, there is a standard called Coventional Commits which describes a standard structure which promotes good practices. See HERE for a useful cheatsheet which I personally reference often.</p>"},{"location":"dev-env/1/#branches-parallel-lines-of-development","title":"Branches: Parallel Lines of Development","text":"<p>Branches allow you to work on new features or bug fixes without affecting the main version of your project. Imagine the \"main\" branch as your stable, working code. When you start a new feature, you create a new branch from <code>main</code>, work on it independently, and then merge it back when it's ready.</p> <pre><code>gitGraph\n    commit id: \"Initial commit\"\n    branch feature-A\n    commit id: \"Work on Feature A\"\n    checkout main\n    branch feature-B\n    commit id: \"Work on Feature B\"\n    checkout main\n    merge feature-A id: \"Merge Feature A\"\n    checkout feature-B\n    commit id: \"More work on Feature B\"\n    checkout main\n    merge feature-B id: \"Merge Feature B\"</code></pre> <p>In GitHub Desktop, you can easily create new branches, switch between them, and see which branch you're currently on using the \"Current Branch\" dropdown.</p> <p>Keep branches purposeful</p> <p>When building software, it's always good to build things in small pieces, rather than chasing perfection in one commit. When building (or maintaining) software, it's generally good practice that each new feature (or bug fix, etc.) is placed on a new branch, and then merged when it's \"good enough\" to join the <code>main</code> branch.</p>"},{"location":"dev-env/1/#the-general-software-development-cycle-with-github","title":"The General Software Development Cycle with GitHub","text":"<p>Here's a typical workflow you'll follow when developing software with Git and GitHub:</p> <ol> <li>Clone a Repository: Get a copy of the project's code onto your local machine.</li> <li>Create a New Branch: Start working on a new feature or fix in isolation.</li> <li>Make Changes &amp; Commit: Write code, save changes, and create commits to record your progress.</li> <li>Push Changes: Upload your local commits to the remote GitHub repository.</li> <li>Create a Pull Request (PR): Propose your changes to be reviewed and merged into the <code>main</code> branch.</li> <li>Review &amp; Discuss: Other team members review your code, suggest improvements, and engage in discussions.</li> <li>Merge: Once approved, your changes are integrated into the <code>main</code> branch.</li> <li>Pull Changes: Update your local repository with the latest changes from the <code>main</code> branch.</li> </ol> <p>Here's a visual representation of this cycle:</p> <pre><code>graph TD\n    A0[Clone/Create Repo]\n    A0 --&gt; A[Start New Feature/Bug Fix]\n    A --&gt; B(Create New Branch);\n    B --&gt; C{Make Changes &amp; Commit};\n    C --&gt; D(Push Branch to GitHub);\n    D --&gt; E(Create Pull Request - PR);\n    E -- Feedback/Changes Required --&gt; C;\n    E -- Approved --&gt; F(Merge PR into Main);\n    F --&gt; G(Delete Feature Branch);\n    G --&gt; H[Pull Latest Main to Local];\n    H --&gt; A;</code></pre>"},{"location":"dev-env/1/#essential-github-desktop-operations-live-demo-focus","title":"Essential GitHub Desktop Operations (Live Demo Focus)","text":"<p>We'll demonstrate these steps live using GitHub Desktop. Pay close attention to the interface elements!</p>"},{"location":"dev-env/1/#cloning-a-repository","title":"Cloning a Repository","text":"<ul> <li>Purpose: To get a copy of an existing GitHub repository onto your computer.</li> <li>GitHub Desktop Steps:<ul> <li>Click \"File\" &gt; \"Clone Repository\".</li> <li>Select the repository from your GitHub.com account, or use the URL if it's a public repository.</li> <li>Choose a local path where you want to save the repository.</li> <li>Click \"Clone\".</li> </ul> </li> </ul>"},{"location":"dev-env/1/#making-changes-committing","title":"Making Changes &amp; Committing","text":"<ul> <li>Purpose: To save a snapshot of your work.</li> <li>GitHub Desktop Steps:<ul> <li>Open the cloned repository folder in your favorite code editor (like VS Code, which we'll cover in Session 2!).</li> <li>Make some changes to a file, or create a new file.</li> <li>Go back to GitHub Desktop. You'll see your modified files listed in the \"Changes\" tab.</li> <li>Staging (Implied in Desktop): In GitHub Desktop, simply checking the box next to a file stages it for the next commit. You can select which changes you want to include.</li> <li>Write a clear and concise Summary (the commit message title) and optionally a Description.</li> <li>Click \"Commit to [current branch name]\".</li> </ul> </li> </ul>"},{"location":"dev-env/1/#branching","title":"Branching","text":"<ul> <li>Purpose: To create an isolated environment for new work.</li> <li>GitHub Desktop Steps:<ul> <li>From the \"Current Branch\" dropdown at the top of the interface, click \"New Branch\".</li> <li>Give your branch a descriptive name (e.g., add-contact-form, fix-login-bug).</li> <li>Click \"Create Branch\". GitHub Desktop will automatically switch you to this new branch.</li> </ul> </li> </ul>"},{"location":"dev-env/1/#pushing-changes","title":"Pushing Changes","text":"<ul> <li>Purpose: To upload your local commits to the remote GitHub repository.</li> <li>GitHub Desktop Steps:<ul> <li>After making commits on your local branch, a \"Publish Branch\" button (for a new branch) or \"Push origin\" button (for an existing remote branch) will appear at the top.</li> <li>Click this button. Your commits will now be visible on GitHub.com.</li> </ul> </li> </ul>"},{"location":"dev-env/1/#submitting-a-pull-request-pr","title":"Submitting a Pull Request (PR)","text":"<ul> <li>Purpose: To propose your changes from your branch to be merged into another branch (usually <code>main</code>). This is where code review happens.</li> <li>GitHub Desktop Steps:<ul> <li>After pushing your branch, GitHub Desktop will often show a prompt to \"Create Pull Request\".</li> <li>Clicking this will take you to GitHub.com, pre-filling much of the PR information.</li> <li>Review: On GitHub.com, you can then add reviewers, describe your changes, and initiate the review process.</li> <li>Reviewing a PR: If someone else creates a PR, you can view the changes, add comments, and approve or request changes directly on GitHub.com.</li> </ul> </li> </ul>"},{"location":"dev-env/1/#git-github-best-practices-dos-and-donts","title":"Git &amp; GitHub Best Practices: Dos and Don'ts \u2705\u274c","text":"<p>To make your Git and GitHub experience smooth and collaborative, keep these best practices in mind:</p>"},{"location":"dev-env/1/#dos","title":"\u2705 Do's:","text":"<ul> <li> Do commit early and often: Make small, focused commits. Each commit should represent a single logical change. This makes it easier to track progress and revert changes if needed.</li> </ul> <p>Making good commit messages</p> <p>At the bare minimum, a commit message must descriptive enough to describe the change. However, there is a standard called Coventional Commits which describes a standard structure which promotes good practices. See HERE for a useful cheatsheet which I personally reference often.</p> <ul> <li> Do write clear, concise commit messages: Your commit message should briefly explain what you changed and why. A good message tells a story. </li> <li> Do use branches for new features/fixes: Always work on a new branch, not directly on <code>main</code>. This keeps <code>main</code> stable and allows for isolated development.</li> <li> Do pull frequently: Regularly pull changes from the <code>main</code> branch to keep your local repository up-to-date and minimize merge conflicts.</li> <li> Do review Pull Requests thoroughly: When reviewing others' code, provide constructive feedback and ask clarifying questions.</li> </ul>"},{"location":"dev-env/1/#donts","title":"\u274c Don'ts:","text":"<p>Data Should be Ephemeral</p> <p>In general, when data is generated programmatically, rather than worrying about saving/archiving the data, it's best to make sure the data can be regenerted easily programmatically. There are some exceptions when data gets larger (and expensive to recompute), however the general concept applies.</p> <ul> <li> Don't commit large data files or binaries: Git is designed for tracking text-based code. Large files like images, videos, or <code>.csv</code> files should generally not be committed directly to Git. Use services like Git LFS (Large File Storage) or external storage solutions for these.</li> <li> Don't commit sensitive information: Never commit API keys, passwords, private keys, or other sensitive credentials directly into your repository. Use environment variables or secure configuration management.</li> <li> Don't force push to shared branches: \"Force pushing\" rewrites commit history. While sometimes necessary on personal branches, it can cause major issues on shared branches like <code>main</code> as it deletes other people's work.</li> </ul> <p>Good Starting Points</p> <p>The community has curated useful <code>.gitignore</code> templates for a variety of projects which you can start from https://github.com/github/gitignore. Furthermore, when creating a new repo with GitHub Desktop, it may even prompt you to start with a suggested .gitignore. </p> <ul> <li> Don't ignore <code>.gitignore</code>: Use a <code>.gitignore</code> file to tell Git which files and directories to ignore (e.g., <code>__pycache__</code>, virtual environments, logs, temporary/sensitive files). This keeps your repository clean.</li> <li> Don't commit commented-out code: If code isn't needed, delete it. Git history preserves it if you ever need to retrieve </li> </ul>"},{"location":"dev-env/1/#optional-practice-further-reading","title":"Optional Practice &amp; Further Reading","text":"<p>To really solidify your understanding, try these exercises and explore the suggested resources. By the start of our next session, you should feel comfortable cloning or creating a repository and contributing to it using GitHub Desktop.</p>"},{"location":"dev-env/1/#optional-practice-exercises","title":"Optional Practice Exercises:","text":""},{"location":"dev-env/1/#create-your-first-repository","title":"Create Your First Repository:","text":"<ul> <li>In GitHub Desktop, go to File &gt; New Repository.</li> <li>Give it a meaningful name (e.g., <code>my-first-git-repo</code>).</li> <li>Initialize it with a <code>README.md</code> file.</li> <li>Make a simple change to the <code>README.md</code> file (e.g., add your name), commit it, and push it to GitHub.com.</li> </ul>"},{"location":"dev-env/1/#clone-a-sample-project","title":"Clone a Sample Project:","text":"<ul> <li>Find a small, open-source project on GitHub (you can search for \"beginner python projects\" or \"hello world repository\").</li> <li>Use GitHub Desktop to clone this repository to your local machine.</li> <li>Make a small, non-breaking change (e.g., fix a typo in a comment, add your name to a CONTRIBUTORS file if one exists).</li> <li>Create a new branch for your change.</li> <li>Commit your change on this new branch.</li> <li>Push your branch to GitHub.</li> <li>Create a pull request (you don't need it to be merged, just demonstrate you can create one).</li> </ul>"},{"location":"dev-env/1/#branching-practice","title":"Branching Practice:","text":"<ul> <li>In one of your local repositories, create three new branches: <code>feature-a</code>, <code>feature-b</code>, and <code>bugfix-c</code>.</li> <li>On <code>feature-a</code>, make one commit.</li> <li>Switch to <code>feature-b</code>, make a different commit.</li> <li>Switch to <code>bugfix-c</code>, make another commit.</li> <li>Push all three branches to GitHub.</li> <li>Try merging <code>feature-a</code> into <code>main</code> (if you feel comfortable, or just observe the merge in GitHub Desktop).</li> </ul>"},{"location":"dev-env/1/#next-steps-questions","title":"Next Steps &amp; Questions","text":"<p>We've covered a lot today! The key takeaway is that GitHub Desktop simplifies these powerful Git concepts into a visual workflow. Practice these operations, and don't hesitate to ask questions.</p>"},{"location":"dev-env/2/","title":"Session 2: Introduction to IDEs with VS Code","text":"<p>Today, we're going to set up our primary coding environment: Visual Studio Code (VS Code). We'll focus on how to use VS Code effectively for Python development, from writing code to debugging, all within a pre-configured DevContainer.</p>"},{"location":"dev-env/2/#introduction-to-visual-studio-code","title":"Introduction to Visual Studio Code \ud83d\ude80","text":"<p>Alternative IDEs</p> <p>There are numerous IDE alternatives out there and many of them can perform have the same functionality, but we will be focused on VS Code in this course.</p> <p>Visual Studio Code (VS Code) is a free, powerful, and highly customizable code editor developed by Microsoft. It's incredibly popular among developers due to its lightweight nature, robust features, and extensive ecosystem of extensions.</p> <p>Why VS Code for Python?</p> <ul> <li>IntelliSense: Smart code completion, signature help, and quick info.</li> <li>Debugging: Integrated debugger for stepping through your code.</li> <li>Extensions: A vast marketplace for Python-specific tools, linters, formatters, and more.</li> <li>Integrated Terminal: Run commands directly within the editor.</li> <li>Git Integration: Seamlessly work with Git (though we'll continue using GitHub Desktop for most Git operations for now).</li> </ul>"},{"location":"dev-env/2/#your-first-steps-a-live-demo-of-vs-code-with-devcontainers","title":"Your First Steps: A Live Demo of VS Code with DevContainers \u2728","text":"<p>Let's begin by familiarizing ourselves with the VS Code interface and opening our lesson materials within a consistent development environment.</p> <p>Using Containers</p> <p>To ensure we all have a similar development experience, we will be making use of Docker and \"devcontainers\". We will go into detail on Docker and devcontainers in a future lesson. Devcontainers are NOT a requirement to use VS Code effectively.</p>"},{"location":"dev-env/2/#installing-vs-code-and-the-remote-development-extension","title":"Installing VS Code and the Remote Development Extension","text":"<ul> <li>Install VS Code: If you haven't already, download and install VS Code from the official website: code.visualstudio.com.</li> <li>Install Remote Development Extension: This powerful extension pack allows you to open any folder in a container.<ul> <li>Open VS Code.</li> <li>Click the \"Extensions\" icon on the left sidebar (looks like four squares, one detached).</li> <li>In the search bar, type \"Remote Development\" (or <code>ms-vscode-remote.vscode-remote-extensionpack</code>)</li> <li>Find the \"Remote Development\" extension pack by Microsoft and click \"Install\". This pack includes the necessary \"Dev Containers\" extension.</li> </ul> </li> </ul>"},{"location":"dev-env/2/#open-the-lesson-folder-in-a-devcontainer","title":"Open the Lesson Folder in a DevContainer \ud83d\udc33","text":"<p>To ensure a consistent development experience, we'll open our lesson folder in a DevContainer. This uses Docker to create a pre-configured environment for you.</p> <p>To Open the Lesson Folder in a Container:</p> <ul> <li>First, ensure you have Docker Desktop installed and running on your machine. And that you have this repository cloned on your computer.</li> <li>Open VS Code then open the command palatte with <code>ctrl+shift+p</code>/<code>cmd+shift+p</code>. Search for the option: <code>Dev Containers: Open Folder in Container</code>.</li> <li>Navigate to this cloned repository. Inside that repository, find the <code>examples/vscode-basic</code> folder. Select this <code>vscode-basic</code> folder and click \"Open\".</li> </ul> <p>Slllooooooowwwww</p> <p>We will cover this concept in the Docker lesson, but just know that the first time you launch the devcontainer it will take significant time to download some asset files. However, future attempts will be much faster due to caching.</p> <ul> <li>VS Code will now set up the container. Once complete, you'll be working inside a fully configured development environment!</li> <li>You'll see the Explorer (left sidebar) populate with your project's files and folders, now running within the DevContainer.</li> </ul>"},{"location":"dev-env/2/#explore-the-interface-and-create-files","title":"Explore the Interface and Create Files","text":"<p>Now that we're inside the DevContainer, let's explore!</p> <ul> <li>The Explorer (Left Sidebar): This shows your project's files and folders.<ul> <li>New File: In the Explorer, hover over the vscode folder and click the \"New File\" icon (a document with a plus sign). Name it <code>hello.py</code>.</li> <li>New Folder: Similarly, click the \"New Folder\" icon.</li> </ul> </li> <li>The Editor Area (Middle): This is where you'll write your code.<ul> <li>Type <code>print(\"Hello from VS Code in a DevContainer!\")</code> into <code>hello.py</code>.</li> </ul> </li> <li>The Integrated Terminal (Bottom): This is your command line, already connected to the environment inside the container.<ul> <li>Open it by clicking \"Terminal\" &gt; \"New Terminal\" (or Ctrl+<code>/Cmd+</code>  backtick).</li> </ul> </li> <li>Run your script: <code>python hello.py</code>. You should see <code>\"Hello from VS Code in a DevContainer!\"</code> printed.</li> </ul>"},{"location":"dev-env/2/#verify-python-extensions-and-interpreter","title":"Verify Python Extensions and Interpreter \ud83d\udc0d","text":"<p>Python Interpreter / IntelliSense not showing?</p> <p>Give it a minute for the extension to install and start properly. If you are noticing issues with the Python extension not working after installation, open the command palatte and search/run: <code>Developer: Reload Window</code>. Also, make sure if you have a python script active within the editor for the <code>Select Interpreter</code> option to appear.</p> <ul> <li>Installing the Python Extensions:<ul> <li>Click the \"Extensions\" icon on the left sidebar.</li> <li>Search for the <code>Python</code> extension pack from Microsoft (<code>ms-python.python</code>) and install it within the devcontainer.</li> <li>You'll notice a \"LOCAL - INSTALLED\" and \"DEV CONTAINER - INSTALLED\" section. The Python and Pylance extensions should appear under \"DEV CONTAINER - INSTALLED\".</li> </ul> </li> <li>Verifying Python Interpreter:<ul> <li>Open a Python file (e.g., <code>main.py</code>).</li> <li>Look at the bottom-right status bar in VS Code. You'll see the Python version (e.g., Python 3.10.12). This confirms which interpreter your container is using.</li> </ul> </li> </ul>"},{"location":"dev-env/2/#write-python-code-with-intellisense","title":"Write Python Code with IntelliSense","text":"<p>Let's write a simple Python function to see IntelliSense in action. </p> <ul> <li>In your vscode folder, create a new file named <code>calculator.py</code>.</li> <li>Type the following code:</li> </ul> calculator.py<pre><code>def add_numbers(a, b):\n    return a + b\n\n\nresult = add_numbers(5, 3)\nprint(f\"The result is: {result}\")\n</code></pre> <ul> <li>As you type add_numbers, notice how VS Code provides code completion suggestions (IntelliSense). This is a huge productivity booster!</li> </ul> <p>Try Importing</p> <p>Try importing a library, such as <code>import numpy as np</code>, and experiment with the different features available - such as typing <code>np.</code> or right clicking <code>np</code>.</p>"},{"location":"dev-env/2/#debug-your-python-code","title":"Debug Your Python Code \ud83d\udc1b\u27a1\ufe0f\u2728","text":"<p><code>print(\"Here!\")</code></p> <p>The act of debugging code by adding statements like <code>print(\"Made it here!\")</code> is an anti-pattern that we want to avoid.</p> <p>Debugging is essential for finding and fixing errors. VS Code's integrated debugger makes this process visual and efficient.</p> <ol> <li>Introduce a Bug: Let's intentionally add a small bug to <code>calculator.py</code>. Change <code>return a + b</code> to <code>return a - b</code>.</li> <li>Set a Breakpoint: Click in the gutter (the space to the left of the line numbers) on the line <code>result = add_numbers(5, 3)</code>. A red dot will appear, indicating a breakpoint.</li> <li>Start Debugging:<ul> <li>Click the \"Run and Debug\" icon on the left sidebar (looks like a bug with a play symbol).</li> <li>Click the \"Run and Debug\" button at the top.</li> </ul> </li> <li>Debug Controls:<ul> <li>Execution will pause at your breakpoint. A floating toolbar will appear:<ul> <li>Step Over (<code>F10</code>): Execute the current line and move to the next.</li> <li>Step Into (<code>F11</code>): If the line is a function call, go inside that function.</li> <li>Step Out (<code>Shift+F11</code>): Finish the current function and return to where it was called.</li> <li>Continue (<code>F5</code>): Run until the next breakpoint or end of the program.</li> <li>Stop (<code>Shift+F5</code>): End debugging.</li> </ul> </li> <li>Use Step Into on the <code>add_numbers</code> call.</li> </ul> </li> <li>Inspect Variables:<ul> <li>In the \"Run and Debug\" sidebar, look at the \"Variables\" section. You can see the values of <code>a</code> and <code>b</code> as you step through the <code>add_numbers</code> function.</li> <li>Notice result is incorrect due to our bug.</li> </ul> </li> <li>Fix the Bug: Stop debugging, change <code>return a - b</code> back to <code>return a + b</code>, and try debugging again to see the correct behavior.</li> </ol> <pre><code>sequenceDiagram\n    participant User\n    participant VSCode_in_Container\n    participant Python_Program_in_Container\n    User-&gt;VSCode_in_Container: Set Breakpoint (e.g., on 'result = ...')\n    User-&gt;VSCode_in_Container: Start Debugging\n    VSCode_in_Container-&gt;Python_Program_in_Container: Run until Breakpoint\n    Python_Program_in_Container-&gt;VSCode_in_Container: Pause\n    VSCode_in_Container-&gt;User: Show current line, variables (a, b)\n    User-&gt;VSCode_in_Container: Step Into ('add_numbers')\n    VSCode_in_Container-&gt;Python_Program_in_Container: Execute 'add_numbers'\n    Python_Program_in_Container-&gt;VSCode_in_Container: Pause (inside function)\n    VSCode_in_Container-&gt;User: Show function variables (a, b)\n    User-&gt;VSCode_in_Container: Step Over\n    VSCode_in_Container-&gt;User: Observe incorrect 'result' in Variables\n    User-&gt;VSCode_in_Container: Stop Debugging\n    User-&gt;VSCode_in_Container: Edit code (fix bug)\n    User-&gt;VSCode_in_Container: Start Debugging (again)\n    VSCode_in_Container-&gt;User: Observe correct 'result'</code></pre>"},{"location":"dev-env/2/#recommended-exercises-further-reading","title":"Recommended Exercises &amp; Further Reading","text":"<p>By the end of this session, and after completing these exercises, you should be comfortable navigating VS Code, writing and running Python code within a DevContainer, and performing basic debugging. This comfort is essential for all future modules.</p>"},{"location":"dev-env/2/#devcontainer-setup-python-environment","title":"DevContainer Setup &amp; Python Environment:","text":"<ul> <li>Ensure you have Docker Desktop installed and running.</li> <li>Open the <code>examples/vscode-basic</code> folder of your Git repository in a DevContainer. Verify that VS Code reopens in the container and that the Python extension is active inside the container.</li> </ul>"},{"location":"dev-env/2/#python-scripting-execution","title":"Python Scripting &amp; Execution:","text":"<ul> <li>Inside the <code>examples/vscode-basic folder</code> (now open in your DevContainer), create a new Python file named <code>greeter.py</code>.</li> <li>Write a Python function that takes a name as an argument and prints a personalized greeting (e.g., <code>\"Hello, [name]!\"</code>).</li> <li>Add code to call this function with different names.</li> <li>Run <code>greeter.py</code> using the integrated terminal in VS Code.</li> </ul>"},{"location":"dev-env/2/#debugging-challenge","title":"Debugging Challenge:","text":"<ul> <li>In your <code>calculator.py</code> file, add a new function that attempts to divide two numbers.</li> <li>Introduce a bug where it might try to divide by zero without handling the error.</li> <li>Use the VS Code debugger to find and identify the line that causes the division by zero error.</li> <li>Implement basic error handling (e.g., using a <code>try-except</code> block) to gracefully handle this scenario.</li> </ul> <p>Try adding a break point within a <code>for</code> loop!</p> <p>Try adding a breakpoint to a line within one python file (such as the <code>add_numbers</code> function) and try to call it from another script.</p>"},{"location":"dev-env/2/#suggested-readings-resources","title":"Suggested Readings &amp; Resources:","text":"<ul> <li>VS Code Official Documentation:<ul> <li>Getting Started with VS Code - A great overview of the editor's features.</li> <li>Python in VS Code - The official tutorial for Python development.</li> <li>Debugging in VS Code - Detailed guide on how to use the debugger.</li> </ul> </li> <li>Dev Containers Documentation:<ul> <li>Develop in a Container - Learn more about what Dev Containers are and how they work.</li> </ul> </li> </ul>"},{"location":"dev-env/2/#next-steps-questions","title":"Next Steps &amp; Questions","text":"<p>Mastering your development environment is a superpower. Take your time with these exercises. If you have any questions or run into issues, please ask! Being comfortable with VS Code for Python development within a DevContainer is a foundational skill for the rest of our bootcamp.</p>"},{"location":"dev-env/3/","title":"Session 3: Docker Fundamentals","text":"<p>Today, we're taking a significant leap into the world of Docker, a technology that's revolutionized how developers build, ship, and run applications.</p>"},{"location":"dev-env/3/#introduction-to-docker","title":"Introduction to Docker \ud83d\udc33","text":"<p>Docker is a platform that uses OS-level virtualization to deliver software in packages called containers. These containers are isolated, lightweight, and portable, allowing applications to run consistently across different environments.</p> <p>Think of it like this: If your code is a recipe, and your computer is a kitchen, then Docker helps you package your recipe (code) and all its necessary ingredients (dependencies) into a perfectly organized meal prep kit (container). This kit can then be reliably cooked (run) in any kitchen, anywhere!</p>"},{"location":"dev-env/3/#containers-vs-virtual-machines-vms","title":"Containers vs. Virtual Machines (VMs)","text":"<p>Before diving deeper, it's crucial to understand why Docker containers are so powerful. Let's compare them to Virtual Machines (VMs), which you might be more familiar with.</p> <p>Virtual Machines (VMs): VMs abstract the entire hardware, requiring a full guest operating system (OS) for each application. This makes them heavier and slower to start.</p> <p>Containers: Containers abstract the operating system itself. They share the host OS's kernel and only package the application and its dependencies, making them extremely lightweight and fast.</p> <p>Here's a visual comparison:</p> <pre><code>graph TD\n    subgraph \"Host Machine (Hardware)\"\n        HW(Hardware)\n    end\n\n    subgraph Virtual Machine Architecture\n        HW --&gt; HostOS_VM(Host OS)\n        HostOS_VM --&gt; Hypervisor(Hypervisor)\n        Hypervisor --&gt; GuestOS1(Guest OS 1)\n        Hypervisor --&gt; GuestOS2(Guest OS 2)\n        GuestOS1 --&gt; App1(App 1)\n        GuestOS2 --&gt; App2(App 2)\n    end\n\n    subgraph Container Architecture\n        HW --&gt; HostOS_Container(Host OS)\n        HostOS_Container --&gt; DockerEngine(Docker Engine)\n        DockerEngine --&gt; Container1(Container 1)\n        DockerEngine --&gt; Container2(Container 2)\n        Container1 --&gt; App1_C(App 1 + Dependencies)\n        Container2 --&gt; App2_C(App 2 + Dependencies)\n    end</code></pre> <p>Key benefits of Docker:</p> <ul> <li>Portability: Runs the same everywhere.</li> <li>Isolation: Applications are isolated from each other and the host system.</li> <li>Efficiency: Uses fewer resources than VMs.</li> <li>Speed: Starts up much faster than VMs.</li> </ul>"},{"location":"dev-env/3/#the-docker-workflow-from-code-to-container","title":"The Docker Workflow: From Code to Container \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>The magic of Docker lies in a straightforward workflow: you define your application's environment, build it into a reusable package, and then run it. Let's walk through this process with a simple Python application.</p> <pre><code>graph LR\n    A[Your Code &amp; Dependencies] --&gt; B(Write Dockerfile);\n    B --&gt; C(Build Docker Image);\n    C --&gt; D(Run Docker Container);\n    D -- Optional --&gt; E(Push Image to Registry);\n    E --&gt; F[Share/Deploy Image];</code></pre>"},{"location":"dev-env/3/#step-1-write-a-dockerfile-defining-your-environment","title":"Step 1: Write a <code>Dockerfile</code> (Defining Your Environment)","text":"<p>A <code>Dockerfile</code> is a text file that contains all the instructions needed to build a Docker image. An image is a read-only template that bundles your application code, runtime, libraries, and environment variables into a single, self-contained unit.</p> <p>Let's set up a simple Python application in our DevContainer:</p> <p>Open the Lesson Folder: Ensure you have the <code>examples/docker-basic</code> folder (from the class repository) open in your DevContainer.</p> <p>Create a Python App:</p> app.py<pre><code>import time\n\nprint(\"Hello from inside the Docker container!\")\nfor i in range(3):\n    print(f\"Counting: {i+1}\")\n    time.sleep(1)\nprint(\"Application finished.\")\n</code></pre> <p>Create a <code>requirements.txt</code>: (Even for simple apps, it's good practice)</p> requirements.txt<pre><code>numpy\npandas\n</code></pre> <p>Create Your <code>Dockerfile</code>: In the same docker folder, create a file named <code>Dockerfile</code> (no extension) and add these instructions:</p> <p>Dockerfile<pre><code># Use an official Python runtime as a parent image (1)\nFROM python:3.9-slim-buster \n\n# Set the working directory in the container (2)\nWORKDIR /app\n\n# Copy the requirements file first to leverage Docker's build cache (3)\nCOPY requirements.txt .\n\n# Install any needed packages specified in requirements.txt (4)\nRUN pip install -r requirements.txt\n\n# Copy your application code into the container (5)\n# This copies app.py from your current (host) directory into '/app' in the image\nCOPY app.py .\n\n# Command to run app.py when the container launches (6)\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <ol> <li>Starts with a base image that already has Python installed. slim-bookworm is a lightweight version.</li> <li>Sets the default directory inside the container for subsequent commands.</li> <li>Copies your requirements.txt file from your local machine (where you run docker build) into the /app directory in the image. This is a crucial step for getting your code and configuration into the container's filesystem.</li> <li>Executes commands during the image build to install Python dependencies.</li> <li>Copies your main application file (app.py) into the /app directory in the image.</li> <li>Specifies the default command to run when a container starts from this image.</li> </ol>"},{"location":"dev-env/3/#step-2-build-the-docker-image","title":"Step 2: Build the Docker Image","text":"<p>Once your <code>Dockerfile</code> is ready, you use the docker build command to create an image.</p> <p>Open the Integrated Terminal in VS Code (ensure you're in the <code>examples/docker-basic</code> directory).</p> <p>Run this command:</p> <pre><code>docker build -t my-python-app .\n</code></pre> <p><code>docker build</code>: The command to initiate an image build.</p> <ul> <li><code>-t my-python-app</code>: Tags the image with a name (<code>my-python-app</code>). This makes it easy to refer to later.</li> <li><code>.</code>: Tells Docker to look for the <code>Dockerfile</code> in the current directory.</li> </ul> <p>Dynamic Context</p> <p>The ability to specifiy where the <code>Dockerfile</code> is allows you to specify \"context\". Meaning, I can point to a <code>Dockerfile</code> in a different folder, while using the context (ie relative pathing) from where you execute the command. You can combine this with the <code>-f</code> option.</p> <p>You can verify the image was created and see its details:</p> <pre><code>docker images\n</code></pre> <p>Building Specific State</p> <p>Note, when you build an image, this usually describes a specific state of the codebase. If you make changes to files which affect the build (ie Dockerfile, requirements, etc.) then you'll have to re-build the image.</p>"},{"location":"dev-env/3/#step-3-run-the-docker-container-and-volume-mounts","title":"Step 3: Run the Docker Container (and Volume Mounts)","text":"<p>An image is a template; a container is a running instance of an image. You use the docker run command to start a container.</p> <p>From the same terminal, run:</p> <pre><code>docker run my-python-app\n</code></pre> <p>You should see your Python script (<code>app.py</code>) execute within the isolated Docker container.</p>"},{"location":"dev-env/3/#understanding-volume-mounts","title":"Understanding Volume Mounts:","text":"<p>While COPY puts files into the image during build time, volume mounts allow you to share a directory from your host machine (your local computer) directly with the running container. This is incredibly useful for:</p> <p>Development: You can edit code on your host machine, and the changes are immediately reflected in the running container without rebuilding the image.</p> <p>Data Persistence: Store data generated by your container on your host machine, so it's not lost when the container stops or is removed.</p> <p>Let's try running our app with a volume mount:</p> <pre><code># Make sure your current terminal directory is 'examples/docker-basic'\ndocker run -v \"$(pwd):/app\" my-python-app\n</code></pre> <ul> <li><code>-v \"$(pwd):/app\"</code>: This is the volume mount.</li> <li><code>$(pwd)</code>: This is a shell command that gets your current working directory on your host machine (e.g., <code>/path/to/your/repo/examples/docker-basic</code>).</li> <li><code>:</code>: Separates the host path from the container path.</li> <li><code>/app</code>: This is the directory inside the container that will be \"mounted\" to your host directory. Now, any changes you make to <code>app.py</code> on your host will instantly be seen by the container if you restart it, without a rebuild!</li> </ul>"},{"location":"dev-env/3/#step-4-optionally-push-the-image-to-a-registry-sharing-your-work","title":"Step 4: Optionally Push the Image to a Registry (Sharing Your Work)","text":"<p>After building your image, you can optionally push it to a Docker registry like Docker Hub. This makes your image available for others to pull and run, or for deployment to cloud platforms.</p> <p>To push an image, you'd first need to tag it with your Docker Hub username and then use docker push:</p> <pre><code># Example: Tag your image with your Docker Hub username\ndocker tag my-python-app yourusername/my-python-app:latest\ndocker push yourusername/my-python-app:latest\n</code></pre> <p>(We won't do this live in this session, but it's an important part of the workflow).</p>"},{"location":"dev-env/3/#docker-hub-a-registry-for-docker-images","title":"Docker Hub: A Registry for Docker Images \ud83c\udf10","text":"<p>Docker Hub is the world's largest library and community for container images. It's a central place where you can find, manage, and share Docker images.</p> <ul> <li>Official Images: Most major software projects (e.g., Python, Node.js, Redis, PostgreSQL) publish official, pre-built images on Docker Hub. These are often the <code>FROM</code> images you'll use in your Dockerfiles.</li> <li>Community Images: Developers worldwide share their own custom images.</li> <li>Exploration: You can search for various images, inspect their details, and sometimes even view their Dockerfiles to understand how they were built.</li> </ul>"},{"location":"dev-env/3/#exploring-docker-hub-jupyter-lab-example","title":"Exploring Docker Hub: Jupyter Lab Example","text":"<p>Jupyter Lab is a popular environment for data science, and instead of installing everything locally, you can simply run a Docker container pre-configured with Jupyter Lab and many common data science libraries.</p> <ul> <li>Go to hub.docker.com.</li> <li>Search for jupyter/jupyterlab.</li> <li>Notice the different tags (versions) available. This illustrates how complex, ready-to-use environments can be easily accessed via Docker Hub.</li> </ul>"},{"location":"dev-env/3/#docker-desktop-your-container-control-panel","title":"Docker Desktop: Your Container Control Panel \ud83d\udcca","text":"<p>While you'll use the terminal for most Docker commands, Docker Desktop (the application you installed) provides a graphical interface to manage your Docker environment. It's a convenient way to visualize and control your containers and images.</p> <ul> <li>Containers: See all your running and stopped containers. You can easily start, stop, restart, and delete them.</li> <li>Logs: View the real-time output (logs) from your running containers, which is critical for debugging.</li> <li>Images: Manage the Docker images stored locally on your machine.</li> <li>Volumes: Inspect and manage any volumes you've created for data persistence.</li> </ul>"},{"location":"dev-env/3/#live-demonstration-the-docker-workflow-docker-desktop-in-action","title":"Live Demonstration: The Docker Workflow &amp; Docker Desktop in Action \ud83c\udfac","text":"<p>We'll perform these steps live together, so follow along in your DevContainer and with the Docker Desktop application.</p> <ul> <li>Verify Docker Desktop is Running: Confirm the Docker Desktop application is active.</li> <li>Navigate in DevContainer: Ensure the <code>examples/docker-basic</code> folder is open in VS Code, inside the DevContainer.</li> <li>Review <code>app.py</code> and <code>requirements.txt</code>: We'll quickly look at the simple Python application and its requirements.</li> <li>Create and Explain <code>Dockerfile</code>: We'll build the <code>Dockerfile</code> step-by-step, explaining each instruction, paying special attention to COPY.</li> <li>Build Image: Execute <code>docker build -t my-python-app .</code> and analyze the build output.</li> <li>List Images: Run <code>docker images</code> to see your new my-python-app image.</li> <li>Run Container (Basic): Execute <code>docker run my-python-app</code> and observe the Python script running.</li> <li>Run Container (with Volume Mount): Execute <code>docker run -v \"$(pwd):/app\" my-python-app</code>.<ul> <li>Demo: While the app is running, quickly edit <code>app.py</code> on your host machine (e.g., change the print message). Restart the container to show the changes are reflected without rebuilding the image. This highlights the power of volume mounts for development.</li> </ul> </li> <li>Explore Docker Desktop:<ul> <li>Open Docker Desktop.</li> <li>Navigate to the \"Containers\" tab to see <code>my-python-app</code> (and potentially other containers).</li> <li>Click on your running container to view its logs.</li> <li>Demonstrate how to stop and restart the container from the GUI.</li> </ul> </li> <li>Explore Docker Hub: Briefly navigate to hub.docker.com and explore pre-built images.</li> </ul>"},{"location":"dev-env/3/#recommended-exercises-further-reading","title":"Recommended Exercises &amp; Further Reading","text":"<p>By the end of this session and after completing these exercises, you should have a solid grasp of the Docker workflow: writing a <code>Dockerfile</code>, building images, running containers with volume mounts, and understanding Docker Hub and Docker Desktop.</p>"},{"location":"dev-env/3/#containerize-a-python-script-with-dependencies","title":"Containerize a Python Script with Dependencies:","text":"<p>In a new subfolder within <code>examples/docker-basic</code>, create a Python script that uses a common library like requests to fetch data from a public API (e.g., a simple \"hello world\" API).</p> <p>Ensure your requirements.txt file lists the requests library.</p> <p>Write a <code>Dockerfile</code> to containerize this Python script.</p> <p>Build the image (e.g., <code>docker build -t my-script-app .</code>).</p> <p>Run the container using a volume mount for your code (e.g., <code>docker run my-script-app</code>).</p> <p>Verify that the script executes correctly inside the container and prints the expected output.</p>"},{"location":"dev-env/3/#customize-a-jupyter-lab-environment","title":"Customize a Jupyter Lab Environment:","text":"<p>In a new subfolder within <code>examples/docker-basic</code> (e.g., <code>jupyter</code>), create a new <code>Dockerfile</code>.</p> <ul> <li>Base Image: Start your Dockerfile from a Jupyter Docker Stack image.</li> <li>Add Custom Packages: Create a <code>requirements.txt</code> file in the same folder as your <code>Dockerfile</code>. Add a couple of Python libraries that aren't typically pre-installed in Jupyter (e.g., folium for mapping, scikit-image for image processing).</li> </ul> requirements.txt<pre><code>folium\nscikit-image\n</code></pre> <ul> <li>Dockerfile Instructions: Add <code>COPY requirements.txt .</code> and <code>RUN pip install -r requirements.txt</code> to your <code>Dockerfile</code> to install these additional packages during the image build.</li> <li>Build the Image: Build your custom Jupyter Lab image (e.g., <code>docker build -t my-jupyter-env .</code>).</li> <li>Run the Container: Run a container from your image, mapping the port: <code>docker run -p 8888:8888 my-jupyter-env</code></li> </ul> <p>Ports</p> <p>This is important for this exercise and anything which involves starting a web app, such as Jupyter notebook or lab. We will revisit this much later, but all you need to know is <code>-p {port on host}:{port in container}</code> is used to publish/expose a port. Since Jupyter normally runs on port <code>8888</code> we need to bind that port to a free port on the <code>localhost</code> (ie our local computer) - in this case the same port. Theoretically, we could bind to any free port ie <code>-p 123456:8888</code>.</p> <ul> <li>Verify Installation:<ul> <li>Open the URL provided in your terminal output (it usually starts with http://127.0.0.1:8888/?token=...) in your browser to access Jupyter Lab.</li> <li>Create a new Python notebook.</li> <li>In a code cell, try to import the libraries you added (e.g., <code>import folium</code> and <code>#!python import skimage</code>). If no errors occur, your custom packages were successfully installed!</li> </ul> </li> </ul> <p>Making notebooks/changes</p> <p>Make changes within the container. Do you notice your files/changes appearing locally?</p> <p>Extra challenge: volume mounts</p> <p>Create a folder called <code>notebooks/</code> in the same directory as the <code>Dockerfile</code>. Then establish a volume mount which maps <code>./notebooks/</code> (locally) to <code>/app/notebooks</code> (in container). Try again to make notebook edits. Notice anything different?</p>"},{"location":"dev-env/3/#explore-docker-desktop-further","title":"Explore Docker Desktop Further:","text":"<p>After running your containers from exercises 1 and 2, use Docker Desktop to:</p> <ul> <li> View the running containers.</li> <li> Explore the Logs, Exec, Files, and Stats tabs.</li> <li> Stop and then start one of the containers from the GUI.</li> <li> Remove a container (after stopping it).</li> <li> View the images you've built under the \"Images\" tab.</li> </ul>"},{"location":"dev-env/3/#suggested-readings-resources","title":"Suggested Readings &amp; Resources:","text":"<ul> <li>Docker Official Documentation:<ul> <li>Get Started with Docker - Docker's official introduction. Focus on the \"Containers and Images\" section.</li> <li><code>Dockerfile</code> Reference - Detailed explanation of all <code>Dockerfile</code> instructions.</li> <li>Use Bind Mounts - Deep dive into how volume mounts work.</li> </ul> </li> <li>Docker Hub:<ul> <li>hub.docker.com - Explore the vast library of Docker images.</li> </ul> </li> <li>Jupyter Docker Stacks:<ul> <li>Selecting an Image - Official documentation for choosing a base Jupyter image.</li> </ul> </li> </ul>"},{"location":"dev-env/3/#next-steps-questions","title":"Next Steps &amp; Questions","text":"<p>Understanding Docker is fundamental for modern development, especially as we move into distributed systems and DevContainers. Experiment with building and running containers. If you encounter any issues or have questions, please reach out! Next, we'll build on this by exploring Docker Compose and DevContainers in more detail.</p>"},{"location":"dev-env/4/","title":"Session 4: Orchestration and Reproducible Environments","text":"<p>In our last session, we mastered Docker by containerizing a simple Python application. We learned how a single <code>Dockerfile</code> can build a reusable image and how to run a container from it.</p> <p>But what if your application isn't a single unit? What if it needs a database, a web server, or another service to function? That's where Docker Compose comes in.</p> <p>Where did my storage go?</p> <p>If you find that you are running out of room after all of the previous docker lessons, try running <code>docker system prune --volumes --all</code>. This will clear all non-running containers, volumes, and caches. This does mean your next builds will take longer.</p>"},{"location":"dev-env/4/#the-need-for-orchestration-why-docker-compose","title":"The Need for Orchestration: Why Docker Compose?","text":"<p>Running a single container with <code>docker run</code> is great, but real-world applications often involve multiple interconnected services. Manually managing each container and its network settings would be cumbersome and prone to error.</p> <p>Docker Compose is a tool for defining and running multi-container Docker applications. With a single YAML file, you can configure your application's services, networks, and volumes, and then launch everything with a single command. It's the \"orchestra conductor\" for your containers.</p> <p>Here's a diagram illustrating the relationship:</p> <pre><code>graph TD\n    A[compose.yml] --&gt; B(Docker Compose)\n    B --&gt; C{\"Service 1 (e.g., Jupyter)\"}\n    B --&gt; D{\"Service 2 (e.g., Redis)\"}\n    C --&gt; E[Container 1]\n    D --&gt; F[Container 2]\n    subgraph Host Machine\n        G[Local Files]\n    end\n    G &lt;--&gt; C\n    G &lt;--&gt; D\n    E &lt;--&gt; F</code></pre> <p>In this model, the <code>compose.yml</code> file is the blueprint for your entire application stack, defining each service and how they should be configured.</p>"},{"location":"dev-env/4/#the-live-demonstration-setting-up-our-environment","title":"The Live Demonstration: Setting Up Our Environment","text":"<p>Let's put this into practice by building a powerful, reproducible development environment. We'll start with a single Jupyter container and then add a Redis service to demonstrate the power of Compose.</p>"},{"location":"dev-env/4/#single-container-environment","title":"Single-Container Environment","text":"<p>First, let's create a development environment with just Jupyter. This shows that you can use Compose even for a single service to manage its configuration.</p> <p>Open the <code>examples/docker-compose-jupyter</code> folder. Inside, you'll find a <code>requirements.txt</code> and a <code>Dockerfile</code>. The <code>Dockerfile</code> simply installs the <code>requirements.txt</code> on top of the base Jupyter image. The main file is <code>compose.yml</code>. Let's look at it.</p> <p>compose.yml<pre><code>services:  # (1)!\n  # Define the Jupyter service\n  jupyter:\n    # You don't need to build the image if you use a pre-built one\n    # image: quay.io/jupyter/minimal-notebook:python-3.12\n    # However, if you want to build your own image:\n    build: # (2)!\n        context: . # Path to the directory containing the Dockerfile (current directory)\n        dockerfile: Dockerfile  # Name of the Dockerfile\n    ports:  # (3)!\n      - \"8888:8888\"\n    volumes:  # (4)!\n      - .:/home/jovyan/app  # COPY current directory to /home/jovyan/app in the container\n    working_dir: /home/jovyan/app  # Set working directory inside the container (5)\n    environment:  # (6)!\n      FOO: bar  # Example environment variable\n    # The following is optional, but useful for Jupyter\n    command: start-notebook.sh --NotebookApp.token=''  # Start Jupyter without a token (7)\n</code></pre></p> <ol> <li><code>services</code>: This is where we define our containers. In this case, we have a single service named <code>jupyter</code>.</li> <li><code>build</code>: Instead of pulling a pre-built image, we are building our custom image from the <code>Dockerfile</code> in the current directory (<code>.</code>).</li> <li><code>ports</code>: The <code>8888:8888</code> line maps port <code>8888</code> on our local machine to port <code>8888</code> inside the container, allowing us to access the Jupyter server in our browser.</li> <li><code>volumes</code>: This is a crucial line for development. It creates a volume mount that synchronizes our local project directory (<code>.</code>) with the <code>/home/jovyan/app</code> directory inside the container. This means any changes we make to our local files will be immediately visible to the container, and vice versa.</li> <li><code>working_dir</code>: Sets the default directory inside the container to <code>/home/jovyan/app</code>.</li> <li><code>environment</code>: Set environment variables for the container. Alternatively, or in tandem, you can specify <code>env_file</code> if you have environment variables stored in a file, i.e. <code>.env</code>.</li> <li><code>command</code>: We are overriding the default startup command to run Jupyter without requiring a token, making access easier for our local development.</li> </ol> <p>Open the integrated terminal in VS Code, navigate to the <code>docker-compose-jupyter</code> folder, and run:</p> <pre><code>docker compose up\n</code></pre> <p>This will build the image and start the container. You'll see the logs, and after a moment, you can access Jupyter Lab at <code>http://localhost:8888</code>.</p> <p>To stop the services, simply <code>ctrl+c</code> to hault the services, then run:</p> <pre><code>docker compose down\n</code></pre> <p>Change my docker environment</p> <p>If you make changes to your docker environment, i.e. changes to the Dockerfile, requirements, etc. Your changes will not reflect automatically. Much like the previous Docker lesson, you must rebuild. Which you can do with <code>docker compose build</code>.</p>"},{"location":"dev-env/4/#multi-container-environment","title":"Multi-Container Environment","text":"<p>Now, let's add a second service to this setup. Most applications need a database or some kind of data store. Instead of a full database, we'll add Redis, an in-memory key-value store, which is perfect for this example.</p> <p>Open the <code>examples/docker-compose-jupyter-redis</code> folder. In this folder, you will find a similar <code>Dockerfile</code> and <code>requirements.txt</code>, but this time the <code>requirements.txt</code> includes the <code>redis</code> Python library. Now, let's examine the <code>compose.yml</code> file.</p> <p>compose.yml<pre><code>services:\n  # Define the Jupyter service\n  jupyter:\n    # You don't need to build the image if you use a pre-built one\n    # image: quay.io/jupyter/minimal-notebook:python-3.12\n    # However, if you want to build your own image, uncomment the lines below\n    build:\n      context: .  # Path to the directory containing the Dockerfile (current directory)\n      dockerfile: Dockerfile  # Name of the Dockerfile\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - .:/home/jovyan/app  # COPY current directory to /home/jovyan/app in the container\n    working_dir: /home/jovyan/app  # Set working directory inside the container\n    environment:\n      FOO: bar  # Example environment variable\n    # The following is optional, but useful for Jupyter\n    command: start-notebook.sh --NotebookApp.token=''  # Start Jupyter without a token\n    networks:  # (2)!\n      - my-network  # Connect to a custom network\n\n  # Compose is very useful to run multiple services together\n  # Here is an example of a simple Redis service\n  redis:  # (1)!\n    image: \"redis:6.0-alpine\"\n    container_name: my-redis\n    ports:\n      - \"6379:6379\"\n    networks:  # (2)!\n      - my-network  # Connect to a custom network\n\nnetworks:\n  my-network:\n    driver: bridge  # Use the default bridge network driver\n</code></pre></p> <ol> <li><code>redis</code> service: We've added a new service. The <code>image</code> is set to <code>redis:6.0-alpine</code>, a lightweight Redis image from Docker Hub.</li> <li><code>networks</code>: This is the most important new section. When Docker Compose runs, it creates a virtual network by default, and services can find each other using their service names as hostnames. By explicitly defining our <code>my-network</code> and assigning both services to it, we ensure that they can communicate with each other. This is how the Jupyter container will be able to find and talk to the Redis container.</li> </ol> <p>Now, open the terminal, navigate to the <code>docker-compose-jupyter-redis</code> folder, and run:</p> <pre><code>docker compose up\n</code></pre> <p>Information Overload!</p> <p>As you increase the number of containers, or add particularly expressive ones, you may notice that your terminal gets flooded with logging statements. You can instead run <code>docker compose up -d</code> which runs the containers detached (in the background). This means you also don't have to <code>ctrl+c</code> to hault the containers, you can just proceed to <code>docker compose down</code>.</p> <p>Docker Compose will now build the Jupyter image and pull the Redis image, and then start both containers at the same time. You will see the logs for both services in the same terminal.</p> <p>Access Jupyter Lab at <code>http://localhost:8888</code>. Open a new notebook and try to connect to the Redis container using the following code:</p> notebooks/redis.ipynb<pre><code>import redis\n\n# Use the service name 'redis' and the default Redis port\nr = redis.Redis(host='redis', port=6379, db=0)\n\n# Set a key-value pair in Redis\nr.set('student_name', 'Alex')\nprint(\"Key 'student_name' set in Redis.\")\n\n# Retrieve the value from Redis\nretrieved_value = r.get('student_name')\n\n# Decode the value from bytes to a string for printing\nif retrieved_value:\n    print(f\"Retrieved value for 'student_name': {retrieved_value.decode('utf-8')}\")\nelse:\n    print(\"Key 'student_name' not found.\")\n</code></pre> <p>If you see messages printed, then it means your <code>jupyter</code> and <code>redis</code> services are correctly communicating with each other on the Docker network. This is a powerful demonstration of multi-container orchestration.</p> <p>To <code>localhost</code> or not to <code>localhost</code></p> <p>Although you may access redis from your local computer at <code>http://localhost:6379</code>, this is not the case for the jupyter container. Notice that the hostname for the redis service is <code>redis</code>, this means from the Jupyter service that it's trying to access redis at <code>http://redis:6379</code>, NOT on the <code>localhost</code>. If you were to specify <code>http://localhost:6379</code>, it will try to resolve something running on <code>6379</code> WITHIN the jupyter container. It's okay if this is \"uncomfortable\" - it'll make a lot more sense when we learn web application development.</p>"},{"location":"dev-env/4/#recommended-practice-exercises","title":"Recommended Practice Exercises:","text":""},{"location":"dev-env/4/#docker-compose-cli","title":"Docker Compose CLI","text":"<p>Read through some of the other docker compose options running <code>docker compose --help</code> in your terminal. Also read through some of the documentation.</p> <p>Investigate and get comfortable with the following commands and their options:</p> <ul> <li><code>up</code></li> <li><code>down</code></li> <li><code>build</code></li> <li><code>run</code></li> <li><code>exec</code></li> </ul> <p>Don't Boil the Ocean</p> <p>Setting up docker compose projects from scratch takes some practice. But know, when it comes down to using docker compose, it really boils down to 3 commands: <code>docker compose build</code>, <code>docker compose up</code>, <code>docker compose down</code>. You can even cut that down to two if you combine the first two using <code>docker compose up --build</code>.</p>"},{"location":"dev-env/4/#explore-and-experiment-with-your-compose-file","title":"Explore and Experiment with Your Compose File:","text":"<ul> <li>Change a port: In <code>compose.yml</code>, change the <code>ports</code> mapping for the <code>jupyter</code> service to something else, like <code>\"8000:8888\"</code>. Rerun <code>docker compose up</code> and access Jupyter at the new port (<code>http://localhost:8000</code>).</li> <li>Add a container name: Add the <code>container_name</code> property to your <code>jupyter</code> service (e.g., <code>container_name: my-jupyter-instance</code>). Rerun Compose and check Docker Desktop to see the new name.</li> <li>Set an environment variable: Add a new environment variable to the <code>jupyter</code> service (e.g., <code>TZ: 'America/New_York'</code>). In your Jupyter notebook, run <code>import os; os.environ.get('TZ')</code> to see if the variable was correctly set.</li> <li>Change the Redis image: In the <code>redis</code> service, change the <code>image</code> to a different version, like <code>\"redis:7.2-alpine\"</code>. Rerun Compose to see the new image being pulled and used.</li> </ul>"},{"location":"dev-env/4/#simple-producerconsumer-with-redis","title":"Simple Producer/Consumer with Redis:","text":"<p>This exercise demonstrates inter-container communication in a more complete way. You'll create one container to act as a \"producer\" and use your existing Jupyter container as a \"consumer.\"</p> <p>First, set up some new files: - Inside the <code>examples/docker-compose-jupyter-redis</code> folder, create a new subfolder called <code>producer</code>. - In this new folder, create a <code>Dockerfile</code>, a <code>requirements.txt</code> file, and a <code>producer.py</code> script.</p> requirements.txt<pre><code>redis\n</code></pre> <ul> <li>A sample python script which that will write to Redis every few seconds. Mocking a data transmitter.</li> </ul> <p>producer.py<pre><code>import redis\nimport time\n\nr = redis.Redis(host='redis', port=6379, db=0)\n\nprint(\"Producer is starting...\")\nwhile True:\n    timestamp = time.time()\n    message = f\"Message sent at {timestamp}\"\n    print(f\"Producer sending: '{message}'\")\n    r.set('latest_message', message)\n    time.sleep(5)\n</code></pre> - Create a Dockerfile based <code>FROM python:3.12-slim-bookworm</code>. Dockerfile<pre><code>FROM python:3.12-slim-bookworm\n# Do this your self!\n</code></pre></p> <ul> <li> <p>Update the <code>docker-compose-jupyter-redis/compose.yml</code> file to include this producer as a new service. Make sure to pay attention to the network settings.</p> </li> <li> <p>In your terminal, navigate to the <code>producer</code> folder and run <code>docker compose up</code>. This will start the producer container, which will continuously send messages to Redis.</p> </li> <li> <p>Consume the signal: Go back to the jupyterlab service in your browser, create a new notebook with the following code.</p> </li> </ul> consumer.ipynb<pre><code>import redis\nimport time\n\nr = redis.Redis(host='redis', port=6379, db=0)\n\nprint(\"Consumer is starting...\")\nwhile True:\n    message = r.get('latest_message')\n    if message:\n        print(f\"Consumer received: '{message.decode('utf-8')}'\")\n    else:\n        print(\"Consumer received: No message yet.\")\n    time.sleep(5)\n</code></pre> <p>This demonstrates that three separate applications can communicate on the same Docker network (Jupyter (Consumer) -&gt; Redis &lt;-- Producer).</p> <pre><code>graph LR\n    A[Redis] &lt;-- Consume Signals --&gt; B[Jupyter]\n    C[Producer] -- Produce Signal --&gt; A</code></pre>"},{"location":"dev-env/4/#suggested-readings-resources","title":"Suggested Readings &amp; Resources:","text":"<ul> <li>Docker Compose Official Documentation: Overview of Docker Compose - Docker's official guide to Compose.</li> <li>Redis-Py Documentation: Quickstart - Learn more about the Python library for Redis.</li> <li>Jupyter Docker Stacks: Using the Docker Stacks - Official documentation for the Jupyter base images.</li> </ul>"},{"location":"dev-env/5/","title":"Building Reproducible Development Environments using DevContainers","text":"<p>In our last two sessions, we mastered the core concepts of Docker and Docker Compose. We learned how to containerize our applications and use a single file to define and run multi-service applications. Today, we'll build on that foundation to create fully configured, shareable, and reproducible development environments using VS Code DevContainers.</p>"},{"location":"dev-env/5/#what-is-a-devcontainer","title":"What is a DevContainer?","text":"<p>A DevContainer is a running Docker container that is fully configured for a specific development task. It includes all the necessary code, runtimes, tools, and extensions for your project.</p> <p>Think of it as a blueprint for a perfect developer setup. When a new team member joins, they don't have to worry about installing Python, Git, or specific VS Code extensions; they just open the project, and VS Code automatically builds and connects to the DevContainer, giving them an identical, ready-to-use environment.</p> <p>This concept combines the power of: - Docker: For creating an isolated and consistent environment. - VS Code: For a rich and user-friendly development experience.</p>"},{"location":"dev-env/5/#the-core-devcontainer-files","title":"The Core DevContainer Files","text":"<p>To create a DevContainer, you typically need two main files:</p> <ul> <li><code>Dockerfile</code>: This defines the container's base environment. It specifies the operating system, installs any necessary software (like Python, Git, or a compiler), and copies over your project's files. It's the same kind of Dockerfile we've already learned about.</li> <li><code>devcontainer.json</code>: This is the magic file. It tells VS Code how to build and configure the container for development. It specifies which Dockerfile to use, which VS Code extensions to install, and any other settings specific to the project.</li> <li>compose.yml (optional):: Optionally, you can point the devcontainer.json to reference an existing (or multiple) Docker Compose configurations.</li> </ul> <p>DevContainers extend existing Docker containers or images, adding a layer of configuration specific to a developer's workflow. This makes it a great way to mirror your development and production environments by basing your DevContainer on the same image you use in production.</p> <p>Here's how it works:</p> <pre><code>graph TD\n    A[\"Base Image (e.g., Python)\"] --&gt; B(Dockerfile adds Dependencies)\n    B --&gt; C{Custom Container Image}\n    C --&gt; D[Running Container]\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n\n    subgraph VS Code DevContainer\n        E[devcontainer.json file]\n    end\n\n    E --&gt; D\n    style E fill:#9ff,stroke:#333,stroke-width:2px\n\n    subgraph Configuration\n        F[VS Code Extensions]\n        G[VS Code Settings]\n    end\n\n    E --&gt; F\n    E --&gt; G</code></pre>"},{"location":"dev-env/5/#live-demonstration-a-basic-devcontainer-example","title":"Live Demonstration: A Basic DevContainer Example","text":"<p>Let's explore a simple DevContainer setup to see how these files work together.</p> <ul> <li> <p>Install the Extension: First, you'll need the Remote Development extension pack. If you don't have it installed already, open the Extensions view in VS Code and search for <code>Remote Development</code> (or <code>ms-vscode-remote.vscode-remote-extensionpack</code>) and install it. This is what allows VS Code to connect to and manage containers.</p> </li> <li> <p>Open the Project: In VS Code, open the <code>examples/vscode-basic</code> folder.</p> </li> <li> <p>Examine the <code>Dockerfile</code>: Take a look at the <code>Dockerfile</code>. It's a standard Dockerfile that sets up a Python environment and installs <code>sudo</code> and <code>git</code>. It's a clean, simple, and self-contained environment.</p> </li> </ul> Dockerfile<pre><code>FROM python:3.12-slim-bookworm\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y sudo git\n\nCOPY ./requirements.txt /requirements.txt\n\nRUN pip install -r /requirements.txt\n</code></pre> <ul> <li>Examine the <code>devcontainer.json</code>: Now, look at the <code>.devcontainer</code> folder. Inside, you'll find <code>devcontainer.json</code>. This file tells VS Code what to do with the Dockerfile.</li> </ul> <p>devcontainer.json<pre><code>{\n    \"name\": \"VSCode Basic Devcontainer\",  // (1)!\n    \"build\": {\n        // Sets the run context to one level up instead of the .devcontainer folder.\n        \"context\": \"..\",  // (2)!\n        // Update the 'dockerFile' property if you aren't using the standard 'Dockerfile' filename.\n        \"dockerfile\": \"../Dockerfile\"  // (3)!\n    }\n}\n</code></pre></p> <ol> <li><code>\"name\"</code>: The name that will appear in VS Code for this container.</li> <li><code>\"context\": \"..\"</code>: The build context is the directory containing the <code>Dockerfile</code>. We use <code>..</code> to point one directory up, to our main project folder.</li> <li><code>\"dockerfile\": \"../Dockerfile\"</code>: Specifies the location of the <code>Dockerfile</code> relative to our project folder.</li> </ol> <p>Alternatives to <code>build</code></p> <p>You don't have to specify a <code>build</code> target in your devcontainer.json, instead you can point to an existing image by <code>{name}:{tag}</code>, a compose.yml file, or even multiple compose.yml files.</p> <ul> <li> <p>Reopen in Container: Now, a prompt should appear in the bottom-right corner of VS Code asking you to \"Reopen in Container.\" Click it. If you don't see it, you can open the Command Palette (<code>F1</code> or <code>Cmd/Ctrl+Shift+P</code>) and select \"Dev Containers: Reopen in Container.\"</p> </li> <li> <p>Observe the Magic: VS Code will now build the container (if it's the first time) and connect to it. Once it's running, you can:</p> <ul> <li>Open the terminal: Notice that the terminal now reflects the DevContainer's environment (e.g., <code>root@...</code>).</li> <li>Run commands: Try running <code>git --version</code> or <code>python --version</code>. These commands work because they were installed inside the container by our <code>Dockerfile</code>.</li> </ul> </li> </ul>"},{"location":"dev-env/5/#recommended-practice-exercises","title":"Recommended Practice Exercises:","text":""},{"location":"dev-env/5/#understand-previous-example","title":"Understand Previous Example","text":"<p>Take a look at the <code>examples/vscode-basic</code> example project, understand how the devcontainer was set up, and try to run it again.</p>"},{"location":"dev-env/5/#create-a-devcontainer-for-the-jupyterredis-project","title":"Create a DevContainer for the Jupyter/Redis Project","text":"<ul> <li>Go to your <code>examples/docker-compose-jupyter-redis</code> folder from the last session.</li> <li>Create a new folder inside it named <code>.devcontainer</code>.</li> <li>Inside the <code>.devcontainer</code> folder, create a new file named <code>devcontainer.json</code>.</li> <li>Your <code>devcontainer.json</code> file needs to be configured to use your existing <code>compose.yml</code> file and attach to the Jupyter service.</li> <li>Your configuration should look something like this:</li> </ul> <p>devcontainer.json<pre><code>{\n    \"name\": \"Jupyter/Redis Dev Environment\",\n    \"dockerComposeFile\": \"../compose.yml\",  // (1)!\n    \"service\": \"jupyter\",  // (2)!\n    \"workspaceFolder\": \"/home/jovyan/app\",  // (3)!\n    \"customizations\": {  // (4)!\n        \"vscode\": {\n            \"extensions\": [\n            \"ms-python.python\",\n            \"ms-toolsai.jupyter\"\n            // Try to add some other extensions!\n            ]\n        }\n    }\n}\n</code></pre></p> <ol> <li><code>dockerComposeFile</code>: Points to your main Compose file.</li> <li><code>service</code>: Specifies the service to which VS Code should attach. This is how VS Code knows to connect to your <code>jupyter</code> container and not your <code>redis</code> container.</li> <li><code>workspaceFolder</code>: Sets the directory inside the container that VS Code should open.</li> <li><code>customizations</code>: This is the most powerful feature for sharing settings. We use it to automatically install the <code>Python</code> and <code>Jupyter</code> extensions for anyone who opens this project in a DevContainer.</li> </ol> <p>Multiple Compose Files</p> <p>Although not shown here, sometimes it is useful to define multiple compose files - one for production and another for development. Since the <code>dockerComposeFile</code> parameter allows you to specify an array, you an actually compose multiple compose files. In other words, start from a \"prod\" definition and override with select \"dev\" specific configurations. We will see this in action in future lessons.</p> <p>Verify Your DevContainer</p> <ul> <li>Now, in VS Code, close the window and re-open the <code>examples/docker-compose-jupyter-redis</code> folder.</li> <li>You should be prompted to \"Reopen in Container.\" Select it.</li> <li>After the container starts, verify the following:<ul> <li>The Python and Jupyter extensions were automatically installed.</li> <li>The integrated terminal is connected to the DevContainer.</li> <li>You can open your Jupyter notebooks and connect to the Redis service as you did in the last lesson.</li> </ul> </li> </ul>"},{"location":"dev-env/5/#suggested-readings-resources","title":"Suggested Readings &amp; Resources:","text":"<ul> <li>VSCode Dev Containers Documentation: https://code.visualstudio.com/docs/devcontainers/containers</li> </ul>"},{"location":"llms/","title":"Large Language Models &amp; AI Tools","text":"<p>This module provides a practical guide to Large Language Model (LLM) technologies and how to use them effectively.</p> <p>Session 1: LLM Fundamentals &amp; Direct API Interaction This session lays the conceptual and technical groundwork for working with Large Language Models. We will cover the basic mechanics of LLMs (tokens, context windows), introduce essential prompt engineering techniques like Zero-shot and Few-shot prompting, and explain the core API parameters that control the model's output, such as temperature and top-k. The primary goal is to bypass frameworks initially and provide students with hands-on experience by executing direct, low-level API calls to the Gemini endpoint in Python, understanding exactly what is happening under the hood before adding complexity.</p> <p>Session 2: Grounding, Structured Output, and RAG Concept This class addresses the limitations of bare LLMs, focusing on how to make their output reliable and predictable. We'll first tackle the problem of hallucination by demonstrating the use of the Google Search tool to ground responses in real-time information, observing the citation metadata. Crucially, we will leverage the students' advanced Python knowledge by showing them how to use Pydantic models to define JSON schemas, forcing the model to return structured, strongly-typed data. The session will conclude with a conceptual introduction to Retrieval-Augmented Generation (RAG) as the principle of grounding the model in private data.</p> <p>Session 3: Frameworks and Retrieval Systems (LangChain &amp; LlamaIndex) Moving from direct API calls, this session justifies and introduces high-level orchestration frameworks, specifically LangChain, for building scalable LLM applications. We will explore the framework's components (Prompts, Models, Parsers) and focus on implementing the Retrieval-Augmented Generation (RAG) pipeline. Students will learn the RAG components in depth: how to load external documents, use Text Splitters to break them into manageable chunks, and use Vector Stores and Retrievers to fetch the most relevant context, demonstrating how to build a basic question-answering system over a private dataset.</p> <p>Session 4: Agentic Workflows and the ReACT Pattern The final session focuses on building a more sophisticated type of LLM application: an intelligent agent. We will distinguish between simple, fixed chains and dynamic agents that use the LLM as a reasoning engine to determine the next action. The core concept taught is the ReACT (Reasoning and Acting) pattern, where the agent decides whether to use a Tool (an external resource, like a calculator or a code function) or use the general LLM to answer. The ultimate goal is for students to learn how to define custom Python functions and wrap them as tools for the agent, enabling it to perform complex, multi-step tasks.</p>"},{"location":"llms/1/","title":"LLM Fundamentals &amp; Direct API Interaction","text":"<p>This session is designed to give you a robust, practical understanding of Large Language Models and, most importantly, to execute your first direct API calls to them. By the end of this class, you'll know the core components of an LLM system and the structure of the request you send and the response you receive.</p>"},{"location":"llms/1/#llms-in-context-generative-vs-discriminative-ai","title":"LLMs in Context: Generative vs. Discriminative AI","text":"<p>To understand Large Language Models (LLMs), we must first place them within the landscape of Machine Learning (ML).</p> <pre><code>flowchart TB\n    AI_ML[AI / ML]\n    AI_ML --&gt; NLP[\"Natural Language Processing (NLP)\"]\n    NLP --&gt; LLMs[\"Large Language Models (LLMs)\"]\n    LLMs --&gt; Gen[Generative LLMs]\n    LLMs --&gt; Disc[Discriminative LLMs]\n    Gen --&gt; GenEx[\"e.g., GPT\u20114, Gemini\"]\n    Disc --&gt; DiscEx[\"e.g., BERT, classification/tagging\"]\n\n    %% Additional subfields (not expanded) connected with dotted lines\n    AI_ML -.-&gt; CV[\"Computer Vision\"]\n    AI_ML -.-&gt; Speech[\"Speech &amp; Audio\"]\n    AI_ML -.-&gt; Robotics[\"Robotics\"]\n    AI_ML -.-&gt; Other[\"Other Subfields (time-series, recommenders, etc.)\"]</code></pre> Type Goal Task Examples Example Model Discriminative AI Classification. Learns to distinguish between existing classes or labels. Spam detection, image classification (Cat/Dog), predicting churn (Yes/No). BERT (Analyzes existing text). Generative AI Creation. Learns patterns to create entirely new content that did not exist before. Creating new text, generating images, writing novel code. Gemini, GPT-4, DALL-E (Creates new content). <p>LLMs are the most advanced subset of Generative AI focused on language. They are fundamentally designed to predict the next token in a sequence, which allows them to \"generate\" fluent and coherent text.</p> <p>Lack of Understanding</p> <p>It's important to remember that generative LLMs don't have any fundamental \"understanding\" (yet), rather the goals is to generate sequence that looks statistically probable. I like to think of it like stirring a bowl of \"alphabet soup\" in the right way that looks like a right answer.</p>"},{"location":"llms/1/#the-llm-engine-tokens-vectors-and-context","title":"The LLM Engine: Tokens, Vectors, and Context","text":"<p>An LLM cannot directly process text; it must convert it into a numerical format. Understanding this process is key to managing prompt length and model output.</p>"},{"location":"llms/1/#tokens-the-basic-unit-of-text","title":"Tokens: The Basic Unit of Text","text":"<p>An LLM reads and predicts tokens, which are the fundamental building blocks of language used by the model.</p> <ul> <li>What are they? Tokens are chunks of text that can represent whole words, subwords, or punctuation. For example, \"unbelievable\" might be broken into the tokens \"un,\" \"believe,\" and \"able.\"</li> <li>Estimation Rule of Thumb: This is crucial for managing your context window:<ul> <li>1 token ~ 3/4 word</li> <li>100 tokens ~ 75 words</li> </ul> </li> </ul> <p>The entire interaction\u2014your prompt, the data, and the model\u2019s response\u2014is calculated in tokens, which directly impacts processing time and cost.</p> <p>OpenAI Tokenizer</p> <p>We can see an example of tokenization using the online OpenAI Tokenizer tool. Try entering the following examples one at a time and note the results.</p> <ul> <li><code>The quick brown fox jumps over the lazy dog.</code></li> <li><code>I'm going to ride the rollercoaster!</code></li> <li><code>It's a beautiful day, isn't it</code></li> <li><code>even_numbers = {i for i in range(10) if i % 2 == 0}</code></li> </ul>"},{"location":"llms/1/#vectors-and-embeddings-the-numerical-language","title":"Vectors and Embeddings: The Numerical Language","text":"<p>Every token is converted into a vector (a list of floating-point numbers) that represents its meaning and relationship to other tokens. This numerical form, called an embedding, is what the model actually uses to understand and process language.  This numerical representation allows the model to perform complex calculations on text.</p> <p></p> <p>Naive Example</p> <p>A very simplistic example is the \"Bag of Words\" approach, which takes your text and generates vectors based on term frequency/prescence within your defined vocabulary. For example, take the example <code>It was the best of times</code>.  The scoring of the document could look as follows:</p> <pre><code>\u201cit\u201d = 1\n\u201cwas\u201d = 1\n\u201cthe\u201d = 1\n\u201cbest\u201d = 1\n\u201cof\u201d = 1\n\u201ctimes\u201d = 1\n\u201cworst\u201d = 0\n\u201cage\u201d = 0\n\u201cwisdom\u201d = 0\n\u201cfoolishness\u201d = 0\n</code></pre> <p>As a binary vector, it would look like: <code>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</code>. Other examples:</p> <pre><code>\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n</code></pre> <p>Granted, modern embedding approaches are more sophisticated, but this should give you a good mental image of the concept.</p>"},{"location":"llms/1/#context-window-the-memory-limit","title":"Context Window: The Memory Limit","text":"<p>Every model has a context window, which is the absolute maximum number of tokens (input + output) it can process in a single request.</p> <ul> <li>This limit dictates how much data, conversation history, and system instructions you can send at once.</li> <li>If your total token count exceeds the context window, the model will fail to process the request, or the output will be truncated.</li> </ul>"},{"location":"llms/1/#prompt-engineering-communicating-with-the-model","title":"Prompt Engineering: Communicating with the Model","text":"<p>Prompt Engineering is the practice of writing effective instructions for an LLM. When interacting with any LLM platform's user interface, your input is structured logically into three parts:</p> Component Purpose Example System Instruction Defines the model's persona, tone, and permanent rules. This sets the stage. \"Act as a pirate who loves Python code.\" User Query The specific question or request the model must fulfill right now. \"Write a short function to calculate area.\" Context/Examples External information the model needs (data, documents, or examples). Example: A JSON object it needs to parse."},{"location":"llms/1/#prompt-patterns","title":"Prompt Patterns","text":"Pattern Description When to Use Zero-Shot The prompt gives no examples. It relies only on the model's pre-trained knowledge. Quick, general questions (e.g., \"Summarize this article.\"). Few-Shot The prompt includes 1-3 pairs of example inputs and outputs to teach the model a specific style or format. Tasks requiring custom formatting (e.g., translating names to a specific JSON structure). <p>Zero Shot Prompt Examples</p> <p>The goal of a Zero-Shot prompt is to rely purely on the model's vast pre-trained knowledge and your explicit instruction. There are no examples provided.</p> Goal System Instruction User Query Expected Behavior Simple Translation You are an expert translator. Your only job is to translate text from English to French. Translate the following: \"The quick brown fox jumps over the lazy dog.\" The model translates the text directly without any formatting fluff. Factual Summary You are a concise history professor. Summarize the key causes of World War I in three concise bullet points. What were the three main causes of World War I? The model uses its knowledge base to extract and format the requested information. Code Interpretation You are a helpful Python assistant. Explain the following code and suggest one improvement. <code>def area(w, h): return w*h</code> The model explains the function calculates area and might suggest adding type hints or docstrings. <p>Few-Shot Prompt Examples</p> <p>The goal of a Few-Shot prompt is to teach the model a specific input/output format, style, or transformation by providing 1-3 working examples within the context. </p> Goal System Instruction User Query Examples Provided in Prompt Tone Transformation Your job is to convert formal technical documentation into the style of a medieval blacksmith. Input: I am designing a new database schema for customer data. Example 1: Input: <code>I need to install the latest dependency</code>. Output: <code>\"Aye, I must hammer the new parts into the bellows.\"</code> Example 2: Input: <code>The current system is too slow.</code> Output: <code>\"Verily, this ancient forge is burdened by sluggish coal.\"</code> Structured Output You are a data extraction bot. Convert the input into a strict JSON format with keys: <code>item</code>, <code>cost</code>, <code>location</code>. Input: A used laptop for $450 in Brooklyn. Example 1: Input: <code>New speakers for $120 in Manhattan.</code> Output: <code>{\"item\": \"New speakers\", \"cost\": 120, \"location\": \"Manhattan\"}</code> Custom Code Style You are a legacy C++ developer. Convert the user's Python code into C++ code that exclusively uses pointers and C-style arrays. Input: <code>def add(a, b): return a + b</code> Example 1: Input: <code>def print_msg(msg): print(msg)</code> Output: <code>void print_msg(char *msg) { printf(\"%s\", msg); }</code>"},{"location":"llms/1/#tips-for-better-prompting","title":"Tips for Better Prompting","text":"<p>A well-engineered prompt is the cheapest way to improve your model's performance. Focus on these elements (you can find more detailed patterns at resources like promptingguide.ai):</p> <ul> <li>Be Explicit with Role and Goal: Don't just ask a question; tell the model who it is (\"You are a senior analyst\") and what it must achieve (\"Summarize the main points into exactly three bullet points\").</li> <li>Use Delimiters: Use clear separators (like triple quotes <code>\"\"\"...\"\"\"</code> or <code>&lt;data&gt;...&lt;/data&gt;</code>) to separate the instructions from the data. This helps the model distinguish what it should process from what it should follow.</li> <li>Give Constraints: Always constrain the output format (e.g., \"Respond in valid JSON,\" \"Keep the answer under 50 words,\" or \"Do not mention the word 'amazing'\").</li> </ul>"},{"location":"llms/1/#direct-api-interaction-controlling-the-output","title":"Direct API Interaction: Controlling the Output","text":"<p>All interactions with LLMs, whether through a simple chat interface or a complex framework, rely on API (Application Programming Interface) calls.</p> <ul> <li>An API defines a set of rules (endpoints, methods, data formats) that allow two pieces of software to talk to each other.</li> <li>In the case of LLMs, your code acts as the client, and the LLM service acts as the server.</li> </ul>"},{"location":"llms/1/#making-the-request-http-and-json","title":"Making the Request: HTTP and JSON","text":"<p>You communicate with the LLM API using the HTTP protocol and sending data formatted as JSON (JavaScript Object Notation).</p> Command Protocol Description <code>cURL</code> Command Line A standard tool for manually testing API endpoints. Requires passing the JSON data and headers (including the API key). <code>requests</code> Python Library The most common Python library for making HTTP requests, simplifying the process of sending JSON payloads."},{"location":"llms/1/#model-parameters","title":"Model Parameters","text":"<p>When you make an API call, you include parameters to control the model's creativity and diversity.</p> <ul> <li><code>temperature</code>: This is the most crucial control. It adjusts the randomness of the token selection. Use a low value (e.g., <code>0.1</code>) for factual or code generation tasks, and a high value (e.g., <code>0.8</code>) for creative or brainstorming tasks.</li> <li><code>top_p</code> (Nucleus Sampling): Limits the selection of the next token to the smallest set of tokens whose cumulative probability exceeds the value of <code>top_p</code>. This is an alternative to <code>temperature</code> for controlling output diversity.</li> <li><code>max_tokens</code>: A hard limit on the length of the model's response. Essential for managing costs and context window size.</li> </ul>"},{"location":"llms/1/#the-payload-structure","title":"The Payload Structure","text":"<p>We will focus on the structure required by the OpenAI Chat Completions API, as it's the standard for many commercial applications. Notice how the concepts from Section 3 map directly to the JSON keys, particularly the System and User roles in the <code>messages</code> array.</p> <pre><code>{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n        // Role: system (Our System Instruction/Persona)\n        {\"role\": \"system\", \"content\": \"You are a concise, professional technical writer.\"},\n\n        // Role: user (Our User Query/Instruction)\n        {\"role\": \"user\", \"content\": \"Explain Git branching in simple terms.\"}\n    ],\n\n    // Generation Parameters (Controlling creativity and length)\n    \"temperature\": 0.2, \n    \"max_tokens\": 1024\n}\n</code></pre>"},{"location":"llms/1/#recommended-exercises-homework","title":"Recommended Exercises &amp; Homework","text":"<p>For this week's homework, you will provision your own API key and use basic Python to execute an authenticated request. This builds a foundational understanding of the underlying HTTP requests that all LLM libraries rely on.</p> <ol> <li> <p>OpenAI API Key Provisioning:</p> <ul> <li>Sign up for an OpenAI Platform account and generate a new API Key. Treat this key like a password.</li> </ul> <p>Keep it Secret, Keep it Safe</p> <p>Treat your API key like a password. DO NOT share it with others you don't trust or commit to GitHub. If you wish to commit your code to Git, one method to keep your key safe by storing it within an untracked environment variable (i.e. <code>.env</code> file) which gets loaded into the script.</p> </li> <li> <p>Hardened API Request: You will use Python's built-in <code>requests</code> library to send a complex payload to the OpenAI Chat Completion endpoint.</p> <ul> <li> <p>Goal: Use Python to send the prompt below and observe the output. Experiment by changing the system prompt, temperature, and other parameters to see the change in the model's behavior.</p> </li> <li> <p>Use the following code as a starting point. Replace <code>YOUR_API_KEY</code> and experiment with the values inside the <code>payload</code> to complete the required tests. Note, you shouldn't require any third party package installs:</p> </li> </ul> Example OpenAI Request<pre><code>import requests\nimport json\n\n# 1. Configuration (REPLACE API KEY)\nAPI_KEY = \"YOUR_API_KEY\"\nAPI_URL = \"https://api.openai.com/v1/chat/completions\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\n# 2. Payload Structure (STARTING POINT)\npayload = {\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n        # SYSTEM MESSAGE: Defines the model's persona and output format rules\n        {\"role\": \"system\", \"content\": \"Act as a professional YAML formatting tool. You MUST only respond with the YAML code and nothing else.\"},\n\n        # USER MESSAGE: The data to be converted\n        {\"role\": \"user\", \"content\": \"Convert this data to YAML: Item: Monitor, Price: 299.99, InStock: true, Store: Midtown\"}\n    ],\n    \"temperature\": 0.1\n}\n\n# 3. Execution (The Request)\ntry:\n    response = requests.post(API_URL, headers=headers, json=payload)\n    response.raise_for_status() # Raise an exception for bad status codes\n\n    # Print the model's response content\n    data = response.json()\n    yaml_output = data['choices'][0]['message']['content']\n    print(\"--- API Call Successful ---\")\n    print(yaml_output)\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"An error occurred during the API call: {e}\")\n    if 'response' in locals():\n        print(f\"Response text: {response.text}\")\n</code></pre> <p>Boilerplate</p> <p>If this looks kinda rough to use (with all the extra boiler plate code to send/recieve/process the request/response), that's because it is. For most practical use cases, I advise against making \"direct\" API calls and instead use official libraries/abstractions -- i.e. OpenAI, LangChain, LlamaIndex -- however, I think it is good to learn and see it at least once.</p> </li> <li> <p>Experimentation and Observation:</p> <ul> <li> <p>Experiment 1 (Persona Shift): Change the <code>system</code> message to: \"You are a friendly, overly enthusiastic motivational speaker who must respond in short, punchy sentences.\" Keep the query the same. What is the difference in the output?</p> </li> <li> <p>Experiment 2 (Creativity Test): Restore the original professional <code>system</code> message, but change the query to: \"Write a short, silly poem about Python code and Docker.\" Change <code>temperature</code> from <code>0.1</code> to <code>0.9</code>. How does the style of the poem change?</p> </li> <li> <p>Experiment 3 (Constraint Test): Restore the original professional <code>system</code> message and set <code>temperature</code> to <code>0.1</code>. Change the <code>user</code> message to: \"Explain why the API key is important, but keep the explanation under 20 words.\" Observe whether the model adheres to the word count constraint.</p> </li> <li> <p>Experiment 4 (OpenAI): Try to use the official OpenAI python library to conduct these experiments.</p> </li> </ul> </li> </ol> <p>By the end of this exercise, you'll have a clear, practical understanding of how every parameter in the API payload controls the LLM's final output.</p>"},{"location":"llms/1/#suggested-readings-resources","title":"Suggested Readings &amp; Resources","text":"<ul> <li>Brief Intro to LLMs</li> <li>History of Large Language Models</li> <li>Transformers, the tech behind LLMs</li> <li>Prompt Engineering Tips and References</li> <li>OpenAI API</li> </ul>"},{"location":"llms/2/","title":"Grounding, Structured Output, and the RAG Concept","text":"<p>Last time, we mastered the mechanics of the LLM API call. Today, we focus on making the response reliable and predictable. We will learn techniques to stop the model from making things up (grounding) and ensure the output is always in a format our code can trust (structured output).</p>"},{"location":"llms/2/#grounding-the-solution-to-hallucination","title":"Grounding: The Solution to Hallucination","text":"<p>The most significant limitation of any LLM is hallucination\u2014the tendency to generate fluent, confident, yet factually incorrect information. The model is a prediction engine, not a database, so it often guesses.</p> <p>The professional fix is Grounding: providing the model with external, verified knowledge and forcing it to base its response on that data.</p> <p>The Hallucination Rule</p> <p>LLMs will prioritize information you provide in the prompt context over their general training data. If you provide context, the model will use it. If you provide conflicting information, the model will typically use the provided context.</p>"},{"location":"llms/2/#grounding-via-manual-context-injection","title":"Grounding via Manual Context Injection","text":"<p>The simplest way to ground an LLM is by including the relevant data directly into the prompt (in the User or System context).</p> <p>Copy/Paste Example: Testing Grounding</p> <p>Try asking your LLM the query below. It should not be able to answer correctly unless you provide the context.</p> Context Block (System/User Role) Query Expected Result CONTEXT: <code>The new protocol for user authentication, known as \"Project Cerberus,\" was implemented on October 15th, 2024. This date supersedes all previous documentation.</code> Query: <code>When was Project Cerberus implemented, and why is this date important?</code> The model bases its answer only on the supplied October 15th date, ignoring its general training data."},{"location":"llms/2/#conceptual-rag-scalable-grounding","title":"Conceptual RAG: Scalable Grounding","text":"<p>While manual context injection works, it doesn't scale. If you have millions of documents, you can't manually find the one sentence the model needs.</p> <p>Retrieval-Augmented Generation (RAG) is the professional, automated architecture that solves the scaling problem of grounding.</p> <p>When do I need RAG?</p> <p>You need RAG anytime your application needs to answer questions based on information that is private, recent, or proprietary\u2014knowledge that the base LLM was not trained on (e.g., your company's policy manuals or yesterday's sales data). Additionally, RAG is useful to bypass context window limitations, like when your knowledge base is larger than the context limits of the chosen model.</p>"},{"location":"llms/2/#the-rag-pipeline","title":"The RAG Pipeline","text":"<p>The RAG workflow has two phases: indexing (prep) and execution (query).</p> <pre><code>graph LR\n    subgraph \"Query Execution (Runtime)\"\n        A[User Query] --&gt; B{Retriever};\n        C[Context Chunks] --&gt; D[Augmented Prompt];\n        D --&gt; E{LLM Generation};\n        E --&gt; F[Grounded Answer];\n    end\n\n    subgraph \"Indexing (Prep)\"\n        G[Company Documents/Data] --&gt; H[Text Splitter];\n        H --&gt; I[Vector Store];\n        I --&gt; C;\n    end\n\n    B --&gt; I;</code></pre>"},{"location":"llms/2/#the-retrieval-challenge-keyword-vs-vector","title":"The Retrieval Challenge (Keyword vs. Vector)","text":"<p>The most important step is the Retriever, which decides which external data is relevant to the user's question.</p> <ul> <li>Keyword Search: Finds documents that contain the exact words used in the query. Weakness: Fails if the user uses synonyms.</li> <li>Vector Search: The modern, powerful approach. It uses embeddings to find data that is semantically similar (meaning similar) to the query, even if the exact words are different. This is how RAG achieves high accuracy.</li> </ul>"},{"location":"llms/2/#structured-output-enforcing-predictability","title":"Structured Output: Enforcing Predictability","text":"<p>Even a factually correct, grounded answer is useless if it's in a messy format. Our code needs to consume reliable JSON or YAML data, not free-form paragraphs.</p>"},{"location":"llms/2/#approach-1-simple-prompting-fast-fragile","title":"Approach 1: Simple Prompting (Fast, Fragile)","text":"<p>The fastest way to get structured output is to explicitly command the model in the prompt to only return a specific format.</p> <p>Copy/Paste Example: Data Extraction (NER)</p> <p>Here, we command the model to extract Named Entities and format them as JSON.</p> System Instruction (Copy/Paste) User Query (Copy/Paste) <code>You are a data extractor. Convert the input text into a strict JSON object with the keys: person_name (string), event_date (string), and location (string). Only output the JSON and nothing else.</code> Input Text: <code>Mr. John Smith traveled from Boston on June 4th to attend a conference in London.</code> <p>Fragility Alert</p> <p>Simple prompting for structured output is fragile. The LLM is generating text, not running a JSON compiler. It can easily introduce small errors (e.g., missing a comma or using single quotes) that will cause your Python JSON parser to crash at runtime.</p>"},{"location":"llms/2/#approach-2-pydantic-enforcement-robust-production-ready","title":"Approach 2: Pydantic Enforcement (Robust, Production-Ready)","text":"<p>The professional solution is to use Pydantic <code>BaseModel</code>s and instruct the API to enforce that structure.</p> <p>Pydantic as a Contract</p> <p>When using Pydantic, your code doesn't parse the model's text output; it sends the Pydantic schema to the API. The API then returns a validated JSON object, which you can load directly into a Pydantic object, guaranteeing its type and structure.</p> <p>We use Pydantic to define the exact structure, and the API enforces it.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n# Define the exact data structure we expect\nclass CustomerRecord(BaseModel):\n    \"\"\"A model for validating extracted customer data.\"\"\"\n    id: int = Field(description=\"The unique numeric ID of the customer.\")\n    name: str = Field(description=\"The customer's full name.\")\n    is_active: bool = Field(description=\"True if the account is currently active.\")\n</code></pre>"},{"location":"llms/2/#recommended-exercises-homework","title":"Recommended Exercises &amp; Homework","text":"<p>Your homework focuses on applying Structured Output and testing the fundamental limits that necessitate the RAG architecture.</p> <ol> <li> <p>RAG Motivation Test (Relevance vs. Noise):</p> <ul> <li>Find a moderately long, single document (e.g., \\(\\approx\\) 600-800 words\u2014a project charter, policy manual, or a long blog post). Ensure the document has lots of surrounding detail (noise).</li> <li>Task: Copy and paste this entire document into your <code>user</code> message. Immediately after, ask a single, highly specific question whose answer is only a short sentence buried deep within the text (e.g., \"What is the policy for using personal headphones in the breakroom?\").</li> <li>Observation: The model will often fail to give the exact, correct answer, or it will give a generic summary, proving that simply including all the text is not enough. The Retriever (which RAG provides) is necessary to pull the needle out of the haystack.</li> </ul> </li> <li> <p>Pydantic Validation (Extraction and Generation):</p> <ul> <li>Using your existing Python API setup and your API key, perform the following two tasks, enforcing the structure using Pydantic and the <code>text_format</code> or <code>response_format</code> parameter (see OpenAI documentation).</li> <li> <p>Task A: Data Extraction (Named Entity Recognition - NER):</p> <ul> <li>Define a Pydantic model called <code>EntityExtraction</code> with fields for <code>person_name</code> (string), <code>event_date</code> (string), and <code>location</code> (string).</li> <li>Send the LLM a paragraph of text containing entities (e.g., \"Ms. Eva Green met with the CEO in Paris on December 12th.\").</li> <li>Force the LLM to extract the data into your <code>EntityExtraction</code> model, and verify that the resulting JSON can be easily loaded into a valid Pydantic object.</li> </ul> </li> <li> <p>Task B: Data Generation (Recipe):</p> <ul> <li>Use the <code>Recipe</code> Pydantic model defined below.</li> <li>Send the LLM a query asking it to \"Generate a recipe for a simple chocolate chip cookie.\"</li> <li>Force the LLM to return the output in the strict <code>Recipe</code> schema. Confirm the <code>ingredients</code> field is successfully returned as a clean Python list.</li> </ul> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nclass Recipe(BaseModel):\n    \"\"\"A model representing a structured recipe.\"\"\"\n    name: str = Field(description=\"The formal name of the dish.\")\n    prep_time_minutes: int = Field(description=\"The estimated preparation time in minutes.\")\n    ingredients: List[str] = Field(description=\"A list of required ingredients.\")\n</code></pre> </li> </ul> </li> </ol> <p>Keep it Secret, Keep it Safe</p> <p>Treat your API key like a password. DO NOT share it with others you don't trust or commit to GitHub. If you wish to commit your code to Git, one method to keep your key safe is by storing it within an untracked environment variable (i.e., a <code>.env</code> file) which gets loaded into the script.</p>"},{"location":"llms/3/","title":"Frameworks and Retrieval Systems (LlamaIndex)","text":"<p>In the previous sessions, we manually crafted API requests and conceptually understood RAG. You likely realized that managing document loading, chunking text, connecting to vector databases, and handling conversation history manually is a lot of \"glue code.\"</p> <p>Today, we introduce Orchestration Frameworks. These are \"batteries-included\" libraries that handle the heavy lifting, allowing you to focus on the logic of your application. We will focus specifically on LlamaIndex, a framework optimized for connecting LLMs to your data.</p>"},{"location":"llms/3/#why-frameworks-the-ecosystem","title":"Why Frameworks? The Ecosystem","text":"<p>Building an LLM app with raw API calls is like building a web server using raw TCP sockets. Possible, but painful and inefficient. Frameworks provide abstractions for:</p> <ul> <li>Data Ingestion: Loading PDFs, CSVs, Notion, SQL, etc.</li> <li>Indexing: Managing how data is split and stored.</li> <li>Retrieval: Finding the right data strategies (keyword vs. vector).</li> <li>Memory: Managing chat history.</li> </ul>"},{"location":"llms/3/#the-landscape","title":"The Landscape","text":"<p>There are several major players, each with a different philosophy:</p> <ul> <li>LlamaIndex: The \"Data Framework.\" Excellent for RAG, indexing, and structured data retrieval.</li> <li>LangChain: The \"Generalist.\" Great for chains, broad integrations, and complex workflows.</li> <li>LangGraph: Built on LangChain, focuses on stateful, cyclic multi-agent workflows.</li> <li>Pydantic AI: A newer, type-safe framework focusing on structured outputs and validation.</li> <li>Haystack: Focused on modular NLP pipelines.</li> <li>DSPy: A radical new approach that \"compiles\" prompts rather than writing them manually.</li> <li>Google AI SDK: Google's native tooling for Gemini integration.</li> <li>...and tons more!</li> </ul> <p>We are focusing on LlamaIndex today because it offers the strong out-of-the-box tools for RAG and data processing.</p> <p>If No Fit...</p> <p>As you get more advanced, you soon realize that some of these frameworks are too bloated or don't support your niche processes. Thus, it is common to build these systems from scratch. However, I highly suggest starting with a framework until you know what you are doing (or know exactly what you need to build).</p>"},{"location":"llms/3/#llamaindex-core-concepts","title":"LlamaIndex Core Concepts","text":"<p>To use LlamaIndex, you need to understand its primary objects:</p> <ol> <li>Documents: The raw data source (text, metadata).</li> <li>Nodes: A \"chunk\" of a document. This is what gets embedded and stored.</li> <li>Index: A data structure composed of Nodes (e.g., a Vector Store Index).</li> <li>Retriever: Fetches relevant Nodes based on a user query.</li> <li>Query Engine: A pipeline that retrieves nodes and sends them to the LLM to generate an answer.</li> </ol> <pre><code>graph TB\n    A[Raw Documents] --&gt;|Load| B(Documents);\n    B --&gt;|Parse/Chunk| C[Nodes];\n    C --&gt;|Embed| D[Vector Index];\n    E[User Query] --&gt;|Search| D;\n    D --&gt;|Retrieve| F[Top-k Relevant Nodes];\n    F --&gt;|Augment| G[LLM Prompt];\n    G --&gt; H[Final Answer];</code></pre>"},{"location":"llms/3/#the-hello-world-of-rag","title":"The \"Hello World\" of RAG","text":"<p>Let's build a RAG system in about 5 lines of code. We will load a local text file and chat with it.</p> <p>Setup Required</p> <p>You need to have the library installed: <code>pip install llama-index</code>. You also need your <code>OPENAI_API_KEY</code> set in your environment variables - or the relevant information for other model providers.</p> <pre><code>import os\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\n\n# 1. Setup (Using OpenAI or Gemini)\n# Settings is a global config object in LlamaIndex\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.1)\n\n# 2. Load Data\n# Assumes you have a folder named 'data' with text files inside\n# SimpleDirectoryReader is a magic tool that handles PDFs, TXT, CSVs automatically\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n# 3. Index Data (This handles chunking and embedding automatically)\nindex = VectorStoreIndex.from_documents(documents)\n\n# 4. Create a Query Engine\nquery_engine = index.as_query_engine()\n\n# 5. Ask a Question\nresponse = query_engine.query(\"What is the summary of the document provided?\")\nprint(response)\n</code></pre> <p>Under the Hood</p> <p>Even though this looks simple, LlamaIndex automatically: 1. Split your text into chunks (default 1024 tokens). 2. Used an embedding model (default OpenAI text-embedding-3-small) to vectorize them. 3. Stored them in an in-memory vector database. 4. On query, searched for the top-k most similar chunks and sent them to GPT-4.</p>"},{"location":"llms/3/#optimization-strategies-improving-rag-performance","title":"Optimization Strategies: Improving RAG Performance","text":"<p>The \"Hello World\" example works for simple demos, but fails in production. To make RAG robust, we optimize at five different stages.</p> <pre><code>graph TD\n    subgraph \"1. Data Prep\"\n    A[Clean &amp; Parse Data]\n    end\n    subgraph \"2. Indexing\"\n    B[Chunking Strategies]\n    C[Metadata Extraction]\n    end\n    subgraph \"3. Retrieval\"\n    D[Hybrid Search]\n    E[Query Routing]\n    end\n    subgraph \"4. Post-Retrieval\"\n    F[Re-Ranking]\n    G[Context Window Injection]\n    end\n    subgraph \"5. Generation\"\n    H[System Prompting]\n    I[Structured Output]\n    end\n    A --&gt; B --&gt; D --&gt; F --&gt; H</code></pre> <p>Updated RAG diagram:</p> <pre><code>graph TB\n    subgraph \"1. Data Prep &amp; Ingestion\"\n    A[Raw Documents] --&gt;|Load| B(Documents)\n    B --&gt;|Parse/Chunk| C[Nodes]\n    end\n\n    subgraph \"2. Indexing\"\n    C --&gt;|Embed| D[Vector Index]\n    end\n\n    subgraph \"3. Retrieval\"\n    E[User Query] --&gt;|Search| D\n    D --&gt;|Retrieve| F[Top-k Candidates]\n    end\n\n    subgraph \"4. Post-Retrieval (Optimization)\"\n    F --&gt;|Re-Rank/Filter| F2[Final Context]\n    end\n\n    subgraph \"5. Generation\"\n    F2 --&gt;|Augment| G[LLM Prompt]\n    G --&gt; H[Final Answer]\n    end</code></pre>"},{"location":"llms/3/#data-creation-preparation","title":"Data Creation &amp; Preparation","text":"<p>Garbage in, garbage out. If your PDF parser mashes headers and footers into the text, the LLM will get confused.</p> <ul> <li>Strategy: Use advanced parsers (like LlamaParse, Docling, or Unstructured) that understand document layout, tables, and images.</li> </ul>"},{"location":"llms/3/#indexing-optimizations-chunking","title":"Indexing Optimizations (Chunking)","text":"<p>How you split your text matters.</p> <ul> <li>Small Chunks: High precision, but might miss context (split by words, sentences, lines, paragraph, etc.).</li> <li>Large Chunks: Good context, but might include \"noise\" that confuses the LLM (split by paragraphs, pages, sections, etc.).</li> <li>Metadata: Adding tags (e.g., <code>{\"year\": 2023}</code>) allows you to filter search results before the vector search, improving accuracy.</li> </ul>"},{"location":"llms/3/#retrieval-optimizations","title":"Retrieval Optimizations","text":"<p>Sometimes the user asks a vague question.</p> <ul> <li>Query Expansion: The system uses an LLM to rewrite the user's query into 3 or 4 better queries, searches for all of them, and combines the results (see HyDE).</li> <li>Hybrid Search: Using Vector Search (semantic) AND Keyword Search (exact match) together.</li> </ul>"},{"location":"llms/3/#post-retrieval-optimization-the-secret-sauce","title":"Post-Retrieval Optimization (The \"Secret Sauce\")","text":"<p>This is often the highest-impact optimization. After fetching the top 10 chunks, we refine them.</p> <ul> <li>Re-ranking: Use a specialized model (like a Cross-Encoder) to re-score the retrieved chunks and pick the absolute best ones.</li> <li>Sentence Window Retrieval: Index small sentences for high-accuracy searching, but when you retrieve them, swap the small sentence out for a larger \"window\" of surrounding text to give the LLM better context.</li> </ul>"},{"location":"llms/3/#generation","title":"Generation","text":"<ul> <li>System Prompting: Telling the LLM strictly \"Answer only based on the context provided.\"</li> <li>Parameter Tuning: Tweaking parameters like temperature, top-p, etc.</li> <li>LLM Selection: Using a model with a larger context window or better reasoning capabilities.</li> </ul>"},{"location":"llms/3/#recommended-exercises-homework","title":"Recommended Exercises &amp; Homework","text":"<p>Your task is to take a baseline RAG system and empirically improve its performance using LlamaIndex features, and then explore structured extraction.</p> <p>The Setup: Download a complex PDF (e.g., a 10-page financial report or a technical manual) and place it in a <code>data</code> folder. Use the \"Hello World\" code from Section 3 as your baseline.</p>"},{"location":"llms/3/#task-1-baseline-vs-optimization","title":"Task 1: Baseline vs. Optimization","text":"<ol> <li>Baseline Test: Ask a specific question (e.g., \"What was the operating margin in Q3?\"). Note the answer. Is it correct? Is it missing details?</li> <li>Optimization Investigation: Choose one of the following advanced techniques to implement. Use the LlamaIndex Optimization Guide to find the code patterns.<ul> <li>Option A (Context): Implement Sentence Window Retrieval. This involves changing the <code>NodeParser</code> to a <code>SentenceWindowNodeParser</code> and adding a <code>MetadataReplacementPostProcessor</code>.</li> <li>Option B (Accuracy): Implement Re-ranking. Install <code>llama-index-postprocessor-colbert-rerank</code> (or similar) and add it to the <code>query_engine</code>'s <code>node_postprocessors</code> list.</li> <li>Try both!</li> </ul> </li> <li>Report: Compare the answer from the optimized system to the baseline.</li> </ol> <p>Debugging RAG</p> <p>If you want to see exactly what text the retriever found, you can inspect the response object: <code>print(response.source_nodes[0].get_content())</code> This is essential for knowing if your retriever failed (found wrong text) or if the LLM failed (couldn't read the text).</p>"},{"location":"llms/3/#task-2-structured-data-extraction","title":"Task 2: Structured Data Extraction","text":"<p>RAG isn't just for chat; it's often used to scrape data from documents.</p> <ol> <li>Goal: Create a script that takes a Resume (PDF or Text) and extracts it into a Pydantic object.</li> <li>Implementation:<ul> <li>Define a Pydantic class <code>Resume</code> with fields like <code>name</code>, <code>skills</code> (list), and <code>experience_years</code> (int).</li> <li>Use LlamaIndex's Structured Data Extraction capabilities. Look up the documentation for <code>program</code> or <code>structured_output</code>.</li> <li>Instead of <code>query_engine.query()</code>, you will use the extraction program to parse the document and return a Python object.</li> </ul> </li> </ol>"},{"location":"llms/3/#suggested-readings-resources","title":"Suggested Readings &amp; Resources","text":"<ul> <li>LlamaIndex Docs: High-Level Concepts</li> <li>Optimization Guide: Production RAG Optimizations</li> <li>Structured Extraction: LlamaIndex Extraction Guide</li> </ul>"},{"location":"module-4/","title":"Module 4: Data Engineering &amp; Distributed Systems","text":""},{"location":"module-4/#data-engineering-fundamentals","title":"Data Engineering Fundamentals \ud83d\udcca","text":"<p>Session 1: This session introduces core data engineering concepts. We'll discuss the differences between ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) and explore their modern applications in data pipelines.</p> <p>Session 2: We'll introduce the Data Medallion Architecture , a foundational pattern for building reliable data lakes. We'll explain the purpose of each layer (Bronze, Silver, Gold) and discuss how data flows through them.</p>"},{"location":"module-4/#databricks-pyspark","title":"Databricks &amp; PySpark \ud83d\udcc8","text":"<p>Session 3: We'll introduce Databricks as a platform for large-scale data processing. We'll explore the basics of Apache Spark and how it distributes data processing across a cluster.</p> <p>Session 4: This is a hands-on session with PySpark. Students will learn to work with Spark DataFrames and perform common data manipulation operations like filter, select, and groupby in a distributed environment.</p>"},{"location":"module-4/#spark-sql-and-data-pipelines","title":"Spark SQL and Data Pipelines","text":"<p>Session 5: SQL basics. Students will write queries in Databricks to join tables, perform aggregations, and transform data.</p> <p>Session 6: Building a simple data pipeline. Students will apply their knowledge to move and transform data between the medallion layers within Databricks using either PySpark or Spark SQL.</p>"},{"location":"module-5/","title":"Module 5: Architecture Design","text":""},{"location":"module-5/#introduction-to-apis","title":"Introduction to APIs","text":"<p>Session 1: Concepts of APIs (REST, GraphQL, gRPC, etc.)</p> <p>Session 2: We'll introduce the FastAPI framework, a modern web framework for building APIs. Building on our advanced Python skills, we'll design and create our first API endpoints.</p>"},{"location":"module-5/#architecture-design-patterns","title":"Architecture Design Patterns","text":"<p>Session 3: We'll explore different system architectures, comparing the benefits and drawbacks of monolithic vs. microservice designs.</p> <p>Session 4: This session covers the principles of event-driven vs. message-driven architectures. We'll discuss how asynchronous communication through message queues can help build scalable and decoupled systems.</p>"},{"location":"module-6/","title":"Module 6: Capstone","text":""},{"location":"reinforcement-learning/","title":"RL Crash Course: From Bellman to Bots","text":"<p>If you would like to follow along with the code, you can find the supplementaly code for this module in the reinforcement-learning GitHub repository including instructions for setting up the environment and running the code.</p>"},{"location":"reinforcement-learning/#part-1-concepts-mathematical-foundations","title":"Part 1: Concepts &amp; Mathematical Foundations","text":"<p>In this section, we will establish the intuition behind Reinforcement Learning (RL), understand when to use it (and when not to), and derive the mathematical equations that power modern agents.</p>"},{"location":"reinforcement-learning/#part-2-building-a-custom-environment-with-gymnasium","title":"Part 2: Building a Custom Environment with Gymnasium","text":"<p>In this section, we transition from theory to practice. We will learn the industry-standard API for RL environments and build a custom simulation of a server room thermostat from scratch.</p>"},{"location":"reinforcement-learning/#part-3-training-with-ppo-and-advanced-frontiers","title":"Part 3: Training with PPO and Advanced Frontiers","text":"<p>In this final section, we will replace our random agent with a state-of-the-art Deep Reinforcement Learning algorithm. We'll train it to perfectly manage our server room, discuss how to tune it, and briefly explore the cutting-edge of RL.</p>"},{"location":"reinforcement-learning/1/","title":"RL Crash Course: From Bellman to Bots","text":""},{"location":"reinforcement-learning/1/#part-1-concepts-mathematical-foundations","title":"Part 1: Concepts &amp; Mathematical Foundations","text":"<p>Objective: In this section, we will establish the intuition behind Reinforcement Learning (RL), understand when to use it (and when not to), and derive the mathematical equations that power modern agents.</p>"},{"location":"reinforcement-learning/1/#the-what-why-why-not-of-rl","title":"The What, Why, &amp; Why Not of RL","text":""},{"location":"reinforcement-learning/1/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>Reinforcement Learning is a subfield of Machine Learning focused on how agents ought to take actions in an environment to maximize some notion of cumulative reward.</p> <p>Unlike Supervised Learning, where the model is provided with an answer key (labels), an RL agent must learn by trial and error. It tries an action, observes the consequence, and adjusts its behavior based on the reward received.</p>"},{"location":"reinforcement-learning/1/#the-core-loop","title":"The Core Loop","text":"<p>The fundamental interaction in RL is a loop between the Agent and the Environment:</p> <ul> <li>State (\\(S_t\\)): The Agent observes the current situation, \\(s \\in S\\).</li> <li>Action (\\(A_t\\)): The Agent chooses an action based on that state. \\(a \\in A\\).</li> <li>Reward (\\(R_{t+1}\\)): The Environment provides feedback (positive or negative).</li> <li>Next State (\\(S_{t+1}\\)): The Environment updates to a new situation, \\(s' \\in S\\).</li> </ul> <p>This cycle continues until a terminal state is reached (e.g., Game Over or Victory).</p> <pre><code>flowchart LR\n    A[Agent] \n    E[Environment]\n\n    A -- \"Action ($$A_t$$)\" --&gt; E\n    E -- \"State ($$S_{t+1}$$)\" --&gt; A\n    E -- \"Reward ($$R_{t+1}$$)\" --&gt; A</code></pre> <p>The Credit Assignment Problem</p> <p>One of the hardest parts of RL is determining which action led to a reward. If a chess agent wins after 50 moves, was the winning move the last one, or a brilliant sacrifice made 20 moves earlier? This is the \"Credit Assignment Problem.\"</p>"},{"location":"reinforcement-learning/1/#when-to-use-rl-the-sweet-spot","title":"When to Use RL (The Sweet Spot)","text":"<p>RL is a powerful tool, but it is not a hammer for every nail. It shines in specific scenarios:</p> <ul> <li>Sequential Decision Making: When the decision you make now affects the options available to you in the future (e.g., Chess, Robotics, Stock Trading).</li> <li>Unknown Optimal Paths: When you know what the goal is (e.g., \"win the game\") but you don't know the exact steps to get there. We don't have the \"labels\" required for supervised learning.</li> <li>Complex Dynamics: When the environment is too complex to model with simple rules (e.g., controlling the plasma inside a fusion reactor).</li> </ul>"},{"location":"reinforcement-learning/1/#when-not-to-use-rl-the-caveats","title":"When NOT to Use RL (The Caveats)","text":"<ul> <li>If Supervised Learning Works: If you have labeled data, use it. RL is notoriously sample inefficient (it takes millions of tries to learn simple things) and computationally expensive.</li> <li>High Cost of Failure: RL agents learn by failing. You do not want an untrained RL agent controlling a real self-driving car on a highway or managing a live power grid. We usually train in simulators first.</li> </ul> <p>Safety First</p> <p>Do not deploy an untrained RL agent directly into a physical system where failure causes damage. Always use a high-fidelity simulator for \"Pre-training\" before moving to \"Sim-to-Real\" transfer.</p> <ul> <li>Reward Hacking: Agents are lazy geniuses. They will exploit flaws in your reward function.<ul> <li>Example: A boat racing agent might learn to spin in circles to collect \"checkpoint\" bonuses infinitely rather than finishing the race.</li> </ul> </li> </ul>"},{"location":"reinforcement-learning/1/#mathematical-foundations","title":"Mathematical Foundations","text":"<p>To solve RL problems, we need to formalize them mathematically. We do this using Markov Decision Processes (MDPs).</p>"},{"location":"reinforcement-learning/1/#the-markov-decision-process-mdp","title":"The Markov Decision Process (MDP)","text":"<p>An MDP is defined by a tuple \\((S, A, P, R, \\gamma)\\):</p> <ul> <li>\\(S\\) (State Space): The set of all possible situations.</li> <li>\\(A\\) (Action Space): The set of all things the agent can do.</li> <li>\\(P\\) (Transition Probability): The laws of physics in the world. \\(P(s'|s,a)\\) is the probability of ending up in state \\(s'\\) if you take action \\(a\\) in state \\(s\\).</li> <li>\\(R\\) (Reward Function): The immediate feedback. \\(R(s,a)\\) is the reward for taking action \\(a\\) in state \\(s\\).</li> <li>\\(\\gamma\\) (Discount Factor): A number between 0 and 1 that determines how much the agent cares about the future.</li> </ul> <pre><code>graph TD\n    S[State $$S$$] -- Action $$a$$ --&gt; P{\"Transition Function $$P(s'|s,a)$$\"}\n    P -- Probability --&gt; S_prime[\"Next State $$(S')$$)\"]\n    P -- Reward Function --&gt; R[\"Reward $$(R)$$\"]</code></pre> <p>The Markov Property</p> <p>The \"Markov Property\" assumes that the current state \\(S_t\\) contains all the information needed to make an optimal decision. You don't need to know the history of how you got there; the \"now\" is sufficient.</p>"},{"location":"reinforcement-learning/1/#the-policy-pi","title":"The Policy (\\(\\pi\\))","text":"<p>The Policy is the agent's brain. It is a mapping from States to Actions. It defines the agent's behavior.</p> <ol> <li>Deterministic Policy: \\(a = \\pi(s)\\)<ul> <li>Plain English: \"If I am in state \\(s\\), I will always do action \\(a\\).\"</li> </ul> </li> <li>Stochastic Policy: \\(\\pi(a|s) = P(A_t=a | S_t=s)\\)<ul> <li>Plain English: \"If I am in state \\(s\\), there is a 70% chance I do action \\(a\\) and 30% I do action \\(b\\).\" (Useful for exploration).</li> </ul> </li> </ol>"},{"location":"reinforcement-learning/1/#the-goal-expected-return-g_t","title":"The Goal: Expected Return (\\(G_t\\))","text":"<p>The agent doesn't just want a high reward now; it wants a high cumulative reward over time. We call this the Return (\\(G_t\\)).</p> <p>However, immediate rewards are usually worth more than distant rewards (due to uncertainty or inflation). We use the Discount Factor (\\(\\gamma\\)) to handle this.</p> \\[G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\] <p>Choosing your Gamma (\\(\\gamma\\))</p> <ul> <li>If \\(\\gamma = 0\\): The agent is myopic (only cares about the very next reward).</li> <li>If \\(\\gamma = 1\\): The agent is far-sighted (cares about all future rewards equally).</li> </ul>"},{"location":"reinforcement-learning/1/#value-functions","title":"Value Functions","text":"<p>To make good decisions, an agent needs to estimate how \"good\" it is to be in a specific situation. We have two functions for this:</p> <ol> <li>The State-Value Function (\\(V(s)\\)): How good is it to be in state \\(s\\)?</li> </ol> \\[V_\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t = s]\\] <p>Plain English Translation</p> <p>The expected return starting from state s, following policy \\(\\pi\\))*</p> <ol> <li>The Action-Value Function (\\(Q(s, a)\\)): How good is it to take action \\(a\\) while in state \\(s\\) right now?</li> </ol> \\[Q_\\pi(s, a) = \\mathbb{E}_\\pi [G_t | S_t = s, A_t = a]\\] <p>Plain English Translation</p> <p>The expected return starting from state s, taking action a, and THEN following policy \\(\\pi\\)</p> <p>The Q-function is critical because if we know \\(Q^*(s, a)\\) (the optimal Q-value), picking the best action is easy: just pick the \\(a\\) with the highest Q-value!</p>"},{"location":"reinforcement-learning/1/#the-bellman-equation","title":"The Bellman Equation","text":"<p>This is the engine of RL. It allows us to define the value of a state recursively.</p> <p>The value of a state is the Immediate Reward plus the Discounted Value of the Next State.</p> \\[V(s) = \\max_a \\left( R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V(s') \\right)\\] <p>This equation tells us that we don't need to simulate the universe until the end of time to know the value of a state; we just need to look one step ahead and rely on our estimate of the next state's value. This \"bootstrapping\" is how algorithms like Q-Learning and PPO work.</p>"},{"location":"reinforcement-learning/1/#next-steps-from-math-to-code","title":"Next Steps: From Math to Code","text":"<p>Now that we understand the MDP (Environment), the Policy (Agent), and the Value Function (Objective), we are ready to implement this in Python.</p> <p>In the next section, we will:</p> <ul> <li>Use <code>gymnasium</code> to create a custom environment (Defining our own \\(S, A, R\\)).</li> <li>Use <code>stable-baselines3</code> to train an agent using PPO to maximize our Reward Function.</li> </ul>"},{"location":"reinforcement-learning/2/","title":"RL Crash Course: From Bellman to Bots","text":""},{"location":"reinforcement-learning/2/#part-2-building-a-custom-environment-with-gymnasium","title":"Part 2: Building a Custom Environment with Gymnasium","text":"<p>Objective: In this section, we transition from theory to practice. We will learn the industry-standard API for RL environments and build a custom simulation of a server room thermostat from scratch.</p>"},{"location":"reinforcement-learning/2/#the-gymnasium-api","title":"The Gymnasium API","text":"<p>Gymnasium vs. OpenAI Gym</p> <p>Gymnasium is the maintained, modern version of the classic OpenAI Gym library. All new RL projects should use <code>gymnasium</code>, as the original <code>gym</code> library is no longer supported by the community!</p> <p>Before an agent can learn, it needs a world to interact with. In Python, the standard framework for creating these worlds is Gymnasium (formerly OpenAI Gym).</p> <p>Gymnasium provides a standardized interface. As long as your environment follows this specific set of rules, any modern RL algorithm can interact with it.</p>"},{"location":"reinforcement-learning/2/#spaces-defining-the-boundaries","title":"Spaces: Defining the Boundaries","text":"<p>How does an agent know what it can see or do? We define the boundaries using <code>gymnasium.spaces</code>. These objects describe the exact shape, bounds, and data types of inputs and outputs.</p> <p>Why do we need to define Spaces?</p> <p>The RL algorithm needs to know the shape and type of the data it will receive (observations) and the data it can send (actions). By defining these spaces, we ensure that our environment and agent can communicate effectively without type errors or mismatched dimensions. Remember, these machine learning algorithms are mathematical functions that expect inputs of a specific shape and type. The spaces act as a contract between the environment and the agent, ensuring they speak the same language.</p> <ul> <li><code>Discrete(n)</code>: A single integer from <code>0</code> to <code>n-1</code>. Used when there are distinct, separate options (e.g., <code>Discrete(3)</code> for moving Left, Right, or Jumping).</li> <li><code>Box(low, high, shape)</code>: An n-dimensional array of continuous numbers (floats). Used for continuous readings (e.g., a camera frame, joint angles, or speedometer).</li> <li><code>MultiDiscrete([n, m])</code>: Multiple discrete actions at once. (e.g., pressing 'A' and 'Up' simultaneously on a controller).</li> <li><code>Dict(...)</code>: A dictionary of simpler spaces. Great for composite observations where variables have different types (e.g., combining a camera frame <code>Box</code> with a scalar health value <code>Discrete</code>).</li> <li><code>Tuple(...)</code>: Similar to a Dict, but structures the spaces as a tuple instead of named keys.</li> </ul> <p>Start with Discrete Actions</p> <p>When building your first environments, try to frame your problem using discrete action spaces if possible. Algorithms generally learn faster and more reliably in finite discrete spaces than in continuous Box spaces where the options are practically infinite!</p>"},{"location":"reinforcement-learning/2/#the-core-methods","title":"The Core Methods","text":"<p>Every Gymnasium environment is a Python class that inherits from <code>gym.Env</code> and implements four main methods:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant A as Agent\n    participant E as Environment (gym.Env)\n\n    A-&gt;&gt;E: reset()\n    E--&gt;&gt;A: observation, info\n\n    loop Every Step\n        A-&gt;&gt;E: step(action)\n        Note over E: Physics Engine &amp; Rules Applied\n        E--&gt;&gt;A: observation, reward, terminated, truncated, info\n    end</code></pre> <ol> <li><code>__init__(self)</code>: Sets up the environment, defining what the agent can see (Observation Space) and what it can do (Action Space).</li> <li><code>reset(self)</code>: Restarts the environment to an initial state and returns the first observation. (Think of this as hitting the reset button on a console). It can also accept a <code>seed</code> for reproducibility and an <code>options</code> dictionary for custom configurations. Returns a tuple: <code>(initial observation, info)</code>.</li> <li><code>step(self, action)</code>: The core physics engine. It takes the agent's action, updates the world, and returns:<ul> <li><code>observation</code>: The new state (\\(S_{t+1}\\)).</li> <li><code>reward</code>: The feedback (\\(R_{t+1}\\)).</li> <li><code>terminated</code>: A boolean indicating if the episode ended due to the rules (e.g., agent crashed).</li> <li><code>truncated</code>: A boolean indicating if the episode ended due to a time limit.</li> <li><code>info</code>: A dictionary for debugging.</li> </ul> </li> <li><code>render(self)</code>: (Optional) Draws the environment to the screen for human viewing.</li> </ol>"},{"location":"reinforcement-learning/2/#hands-on-the-server-room-thermostat-advanced","title":"Hands-On: The Server Room Thermostat (Advanced)","text":"<p>Let's build a custom environment.</p> <p>Real World Case Study: Server Room Thermostat</p> <p>This is a toy model of a real-world problem. Data centers consume massive amounts of energy, and cooling is a huge part of that cost. An intelligent thermostat could learn to pre-cool the room just before a big job starts, saving energy while keeping the servers safe.</p> <p>The Scenario: You manage a server room. Instead of constant random heat, heat is generated by Jobs. Jobs are submitted randomly, one at a time, and require a certain number of CPUs for a few timesteps. More CPUs generate more heat. Your agent controls a thermostat and must keep the room temperature as close to 20\u00b0C as possible.</p> <p>Why are we adding jobs?</p> <p>Because the agent can see the active processes, it can learn to pre-emptively cool the room when a massive job arrives, rather than just reacting after the temperature has already spiked. This teaches the foundation of predictive action!</p>"},{"location":"reinforcement-learning/2/#step-1-defining-the-mdp-state-action-reward-termination","title":"Step 1: Defining the MDP (State, Action, Reward, Termination)","text":"<p>Before writing code, we must translate our real-world problem into the mathematical framework we learned in Part 1.</p> <pre><code>flowchart TD\n    S[State: Temp &amp; CPUs] --&gt; A{Action: Cool / None / Heat}\n    A --&gt; |Apply Action| E[Environment: Job Processing &amp; Heat]\n    E --&gt; R[\"Reward: -abs(Temp - 20)\"]\n    E --&gt; S2[New State]\n    E --&gt; D{Done?}\n    D --&gt; |Yes: Temp &gt; 90 or &lt; 5| F[Terminate: Failure]\n    D --&gt; |Yes: Step &gt;= 60| T[Truncate: Success]\n    T --&gt; R2[\"Reward: +100 survived\"]\n    D --&gt; |No| S2</code></pre> <ul> <li>Action Space (\\(A\\)): The agent can make 3 choices: Cool down (-1), Do nothing (0), Heat up (+1). We will use a <code>Discrete(3)</code> space.</li> <li>Observation Space (\\(S\\)): The agent reads TWO things of completely different types: the current temperature (a continuous float from 0.0\u00b0C - 100.0\u00b0C) and the number of active CPUs currently running a job (an integer from 0 - 100). We will use a composite <code>Dict</code> space containing a <code>Box</code> and a <code>Discrete</code> space.</li> <li>Reward (\\(R\\)): The agent gets punished the further the temperature strays from the target of 20\u00b0C. <code>Reward = -abs(Current_Temp - 20)</code>. And a large bonus reward of <code>+100</code> for surviving the entire episode without melting or freezing the servers.</li> </ul> <p>Termination vs. Truncation</p> <p>The difference between these two is massive to an RL algorithm!</p> <ul> <li>Termination tells the agent: \"This was a failure state, avoid this behavior entirely.\"</li> <li>Truncation tells the agent: \"Time ran out for this simulation, but the state itself wasn't necessarily bad.\"</li> </ul> <ul> <li>Termination &amp; Truncation (Done Conditions): We need to know when the episode ends.<ul> <li>Termination (Failure): If the temperature hits 90.0\u00b0C (servers melt) or drops to 5.0\u00b0C (servers freeze), the episode immediately ends in failure.</li> <li>Truncation (Success): If the agent manages the room successfully for 60 timesteps without a failure, the episode ends successfully (time limit reached).</li> </ul> </li> </ul>"},{"location":"reinforcement-learning/2/#step-2-writing-the-code","title":"Step 2: Writing the Code","text":"<p>Let's write the actual Python code for our environment.</p> custom_env.py<pre><code>import random\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom rich import print\n\nclass ServerRoomEnv(gym.Env):\n    \"\"\"\n    Custom Environment that follows gym interface.\n    The agent must keep the server room temperature near 20\u00b0C \n    by predicting heat generated by random CPU jobs.\n    \"\"\"\n    def __init__(self, verbose=False):\n        super(ServerRoomEnv, self).__init__()\n        self.verbose = verbose\n\n        # Target temperature\n        self.target_temp = 20.0\n\n        # Actions: 0 = Cool, 1 = Do Nothing, 2 = Heat\n        self.action_space = spaces.Discrete(3)\n\n        # Observation: Dictionary of Temp (Box) and Active CPUs (Discrete)\n        # Temp ranges from 0-100 (float), CPUs range from 0-100 (int)\n        self.observation_space = spaces.Dict(\n            {\n                \"temperature\": spaces.Box(\n                    low=0.0, \n                    high=100.0, \n                    shape=(1,), \n                    dtype=np.float32\n                ),\n                \"cpus\": spaces.Discrete(101) # Allows integer values from 0 to 100\n            }\n        )\n\n        # State variables\n        self.current_temp = 20.0\n        self.active_cpus = 0\n        self.job_timer = 0  # Tracks how long the current job lasts\n\n        self.episode_length = 60 # 60 steps per episode\n        self.current_step = 0\n\n    def _get_obs(self):\n        \"\"\"Helper method to format the observation dictionary.\"\"\"\n        return {\n            \"temperature\": np.array([self.current_temp], dtype=np.float32),\n            \"cpus\": int(self.active_cpus)\n        }\n\n    def reset(self, seed=None, options=None):\n        \"\"\"Restarts the environment for a new episode.\"\"\"\n        super().reset(seed=seed)\n\n        # Start at a random temperature near the target\n        self.current_temp = random.uniform(18.0, 22.0)\n        self.active_cpus = 0\n        self.job_timer = 0\n        self.current_step = 0\n\n        return self._get_obs(), {}\n\n    def step(self, action):\n        \"\"\"Applies the action, processes jobs, and calculates the reward.\"\"\"\n        self.current_step += 1\n\n        # 1. Apply Agent's Action Effects\n        if action == 0:   # Cool\n            self.current_temp -= 3.0\n        elif action == 1: # Do Nothing\n            pass\n        elif action == 2: # Heat\n            self.current_temp += 3.0\n\n        # 2. Process Job Lifecycle\n        if self.job_timer &gt; 0:\n            # Job is currently running\n            self.job_timer -= 1\n            if self.job_timer == 0:\n                self.active_cpus = 0 # Job finished\n        else:\n            # No active job. 30% chance a new job arrives this step.\n            if random.random() &lt; 0.30:\n                self.active_cpus = random.randint(10, 80) # Requires 10-80 CPUs\n                self.job_timer = random.randint(3, 8)     # Lasts 3-8 timesteps\n\n        # 3. Apply Environmental Heat (Calculated from active CPUs + slight noise)\n        # Each active CPU adds 0.05 degrees, plus some minor fluctuation\n        cpu_heat = (self.active_cpus * 0.05) \n        ambient_noise = random.uniform(-0.5, 0.5)\n        self.current_temp += (cpu_heat + ambient_noise)\n\n        # Clip temperature to stay within our defined bounds\n        self.current_temp = np.clip(self.current_temp, 0.0, 100.0)\n\n        # 4. Calculate Reward (Negative distance from target)\n        reward = -abs(self.current_temp - self.target_temp)\n\n        # 5. Check for Termination / Truncation\n        terminated = False\n\n        # Terminate if the servers melt or freeze (Failure condition)\n        if self.current_temp &gt;= 90.0:\n            if self.verbose:\n                print(f\"\\n\ud83d\udd25 [bold red]BOOM! Servers melted at {self.current_temp:.1f}\u00b0C! You exploded![/bold red] \ud83d\udd25\")\n            terminated = True\n        elif self.current_temp &lt;= 5.0:\n            if self.verbose:\n                print(f\"\\n\u2744\ufe0f [bold blue]ICE AGE! Servers froze solid at {self.current_temp:.1f}\u00b0C! Welcome to Hoth![/bold blue] \u2744\ufe0f\")\n            terminated = True\n\n        # Truncate if we hit the maximum time limit (Success condition)\n        truncated = bool(self.current_step &gt;= self.episode_length)\n        if truncated and not terminated:\n            if self.verbose:\n                print(\"\\n\u2705 [bold green]SHIFT OVER! You survived the day without destroying the server room.[/bold green] \u2705\")\n            reward += 100.0 # Bonus for surviving the whole episode\n\n        # 6. Format the return values\n        info = {}\n        return self._get_obs(), reward, terminated, truncated, info\n\n    def render(self):\n        \"\"\"Simple text printout of the environment state.\"\"\"\n        print(f\"Step: {self.current_step:02d} | Temp: {self.current_temp:.2f}\u00b0C | Active CPUs: {self.active_cpus}\")\n</code></pre>"},{"location":"reinforcement-learning/2/#step-3-testing-the-environment-with-a-random-agent","title":"Step 3: Testing the Environment with a Random Agent","text":"<p>Before we apply advanced AI to solve this, we must ensure the physics engine works. We can test it by letting an agent take completely random actions.</p> custom_env.py<pre><code>if __name__ == \"__main__\":\n    # Instantiate our new environment\n    env = ServerRoomEnv(verbose=True) # Set verbose to True to see detailed output\n\n    # Reset the environment to start\n    obs, info = env.reset()\n    print(f\"Starting Temperature: {obs['temperature'][0]:.2f}\u00b0C | Starting CPUs: {obs['cpus']}\")\n\n    episodes = 1\n    for ep in range(episodes):\n        obs, info = env.reset()\n        done = False\n        score = 0\n\n        while not done:\n            # Sample a random action from the action space (0, 1, or 2)\n            action = env.action_space.sample()\n            # Force the same action\n            # action = 1 # Do Nothing (for testing)\n            # Replace with your policy function later\n            # action = my_cool_policy(obs)\n            # Add keyboard input for manual control (optional)\n            # action = int(input(\"Enter action (0=Cool, 1=Do Nothing, 2=Heat): \"))\n\n            # Step the environment forward\n            obs, reward, terminated, truncated, info = env.step(action)\n            score += reward\n\n            env.render()\n\n            # Episode ends if terminated (bounds exceeded) or truncated (time up)\n            done = terminated or truncated\n\n        print(f\"Episode {ep + 1} finished with Total Reward: {score:.2f}\")\n\n    # Always close the environment when done\n    env.close()\n</code></pre> <p>Example output:</p> <pre><code>Starting Temperature: 21.14\u00b0C | Starting CPUs: 0\nStep: 01 | Temp: 26.61\u00b0C | Active CPUs: 58\nStep: 02 | Temp: 32.63\u00b0C | Active CPUs: 58\nStep: 03 | Temp: 32.98\u00b0C | Active CPUs: 58\nStep: 04 | Temp: 32.85\u00b0C | Active CPUs: 58\nStep: 05 | Temp: 36.06\u00b0C | Active CPUs: 58\nStep: 06 | Temp: 38.90\u00b0C | Active CPUs: 0 \n...\n\ud83d\udd25 BOOM! Servers melted at 93.1\u00b0C! You exploded! \ud83d\udd25\nStep: 59 | Temp: 93.07\u00b0C | Active CPUs: 57\nEpisode 1 finished with Total Reward: -2398.05\n</code></pre> <p>If you run this code, you will notice the <code>Active CPUs</code> periodically spike up. A random agent will just guess, leading to huge temperature swings. Eventually, you'll see one of our fatal print statements trigger when the random agent inevitably freezes or melts the servers!</p> <p>Next Steps: In the final section, we will train a Deep Reinforcement Learning algorithm (PPO). The neural network will learn the correlation between <code>Active CPUs</code> and rising temperature, allowing it to pre-cool the room exactly when a massive 80-CPU job is submitted!</p>"},{"location":"reinforcement-learning/3/","title":"RL Crash Course: From Bellman to Bots","text":""},{"location":"reinforcement-learning/3/#part-3-training-with-ppo-and-advanced-frontiers","title":"Part 3: Training with PPO and Advanced Frontiers","text":"<p>Objective: In this final section, we will replace our random agent with a state-of-the-art Deep Reinforcement Learning algorithm. We'll train it to perfectly manage our server room, discuss how to tune it, and briefly explore the cutting-edge of RL.</p>"},{"location":"reinforcement-learning/3/#training-a-custom-environment-with-ppo","title":"Training a Custom Environment with PPO","text":"<p>Writing RL algorithms from scratch is highly prone to bugs and sometimes very complex. Instead, the industry relies on robust, optimized libraries. We will use Stable-Baselines3 (SB3), which is built on PyTorch and integrates perfectly with Gymnasium. Supported by Stable-Baselines3 algorithms as of 2026/02/29 </p> Name Box Discrete MultiDiscrete MultiBinary Multi Processing ARS 1 \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f A2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f CrossQ 1 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f DDPG \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f DQN \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f HER \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f PPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f QR-DQN 1 \u274c \ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f RecurrentPPO 1 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f SAC \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f TD3 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f TQC 1 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f TRPO 1 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Maskable PPO 1 \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f <p>RL Libraries</p> <p>There are libraries out there besides Stable-Baselines3 for Deep RL, such as RLlib (my preferred), Torch RL, CleanRL, SKRL, and Tianshou. Recommend checking out others if you want to explore different algorithms or need features not supported by SB3.</p>"},{"location":"reinforcement-learning/3/#enter-ppo-proximal-policy-optimization","title":"Enter PPO (Proximal Policy Optimization)","text":"<p>We are going to use PPO. You don't need to understand the underlying calculus right now, but you should recognize how it maps to the concepts we covered in Part 1:</p> <p>Connecting PPO to the Math</p> <ul> <li>The Objective: PPO's goal is to find the optimal Policy (\\(\\pi\\)) that maximizes the Expected Return (\\(G_t\\)).</li> <li>The Brain: It uses a Neural Network to approximate the State-Value Function (\\(V(s)\\)) to guess how good a state is, and another network to output the Action probabilities.</li> <li>\"Proximal\": It limits how much the agent's brain can change in a single update. This prevents the agent from forgetting everything it learned just because it had one lucky (or unlucky) episode.</li> </ul> <pre><code>flowchart LR\n    subgraph Stable-Baselines3\n        P[PPO Algorithm] &lt;--&gt; |Updates| NN[Neural Network Policy]\n    end\n    subgraph Gymnasium\n        E[ServerRoomEnv]\n    end\n\n    NN -- \"Action ($$A_t$$)\" --&gt; E\n    E -- \"Observation ($$S_{t+1}$$)\" --&gt; P\n    E -- \"Reward ($$R_{t+1}$$)\" --&gt; P</code></pre>"},{"location":"reinforcement-learning/3/#writing-the-training-loop","title":"Writing the Training Loop","text":"<p>Let's train an agent to manage our <code>ServerRoomEnv</code>. You'll need to install the library: <code>pip install stable-baselines3[extra]</code>.</p> train.py<pre><code>from stable_baselines3 import PPO\n\nfrom custom_env import ServerRoomEnv  # Import your custom environment\n\n# 1. Instantiate the Environment\n# Optional: Pass hyperparameters to the environment constructor if needed (e.g., target_temp=22)\nenv = ServerRoomEnv(verbose=True)  # Set verbose to True to see detailed output\n\n# 2. Instantiate the PPO Agent\n# NOTE: Because our observation space is a Dict, we MUST use \"MultiInputPolicy\"\n# If it was just a Box, we would use \"MlpPolicy\"\nmodel = PPO(\"MultiInputPolicy\", env, verbose=1)\n\nprint(\"Starting training...\")\n# 3. Train the Agent! (10,000 steps is very fast, usually takes &lt; 1 minute)\nmodel.learn(total_timesteps=10_000)\nprint(\"Training finished!\")\n\n# 4. Save the \"Brain\"\nmodel.save(\"/path/to/server_room_ppo_model\")\n</code></pre> <p>Training Time</p> <p>The above code will train for 10,000 steps, which is usually enough for a simple environment like ours. However, more complex environments may require hundreds of thousands or even millions of steps to learn effectively. You can adjust <code>total_timesteps</code> as needed. Also, training time depends on your hardware. If you have a GPU, PPO can take advantage of it to speed up training. By default, the code will use CPU, which is fine for our simple environment. See Pytorch documentation for GPU setup.</p>"},{"location":"reinforcement-learning/3/#tuning-the-agent-hyperparameters","title":"Tuning the Agent (Hyperparameters)","text":"<p>If your agent isn't learning, you rarely rewrite the algorithm. Instead, you tune the Hyperparameters. Think of these as the dials and knobs on the back of the AI's brain.</p> <p>Crucial Hyperparameters in PPO</p> <ul> <li>Learning Rate (<code>learning_rate</code>): How big of a step the neural network takes when updating. If it's too high, the agent learns erratically and forgets past lessons. If too low, training takes forever. (Default: 0.0003)</li> <li>Discount Factor (gamma / \\(\\gamma\\)): We saw this in Part 1! A \\(\\gamma\\) of 0.99 means the agent cares deeply about the distant future. A \\(\\gamma\\) of 0.5 makes the agent myopic (only cares about immediate heat spikes).</li> <li>Network Architecture (net_arch): You can make the neural network deeper or wider. If your environment is very complex, a small default network might not have enough \"neurons\" to grasp the patterns.</li> </ul> <p>Recommend using your favorite hyperparameter tuning library (Optuna, Ray Tune, Weights &amp; Biases Sweeps) to automate this process. You can even use a simple grid search if you have the computational resources. And don't forget to track your experiments with tools like Weights &amp; Biases, MLflow, or TensorBoard to visualize how different hyperparameters affect learning!</p>"},{"location":"reinforcement-learning/3/#evaluating-the-trained-agent","title":"Evaluating the Trained Agent","text":"<p>Let's load our trained brain and see how it performs compared to the random agent from the previous section.</p> evaluate.py<pre><code>from stable_baselines3 import PPO\nfrom custom_env import ServerRoomEnv\n\n# Load the trained model\nmodel = PPO.load(\"/path/to/server_room_ppo_model\")\nenv = ServerRoomEnv()\n\n# Enjoy a 5-episode test drive\nfor ep in range(5):\n    obs, info = env.reset()\n    done = False\n    score = 0\n\n    print(f\"\\n--- Episode {ep+1} ---\")\n    while not done:\n        # Instead of random.sample(), we ask our trained model for the best action!\n        # _states is used for recurrent policies (LSTMs), which we aren't using here.\n        action, _states = model.predict(obs, deterministic=True)\n\n        obs, reward, terminated, truncated, info = env.step(action)\n        score += reward\n        env.render()\n\n        done = terminated or truncated\n\n    print(f\"Total Reward: {score:.2f}\")\n\nenv.close()\n</code></pre> <p>If training was successful, you will see the agent intelligently pre-cooling the room the moment <code>Active CPUs</code> spike, resulting in zero melted servers and a high score!</p> <p>Don't Panic if it Doesn't Work at First</p> <p>RL can be very finicky. If your agent is still melting servers, try tuning the hyperparameters, increasing training time, or even simplifying the environment to make it easier to learn. Debugging RL often involves a lot of trial and error, so don't get discouraged! Also note, the environment itself is not perfectly implemented (e.g., the physics of cooling might be too harsh), so you may need to adjust the environment's parameters to make it learnable. You may want to:</p> <pre><code>- Include job duration/timing info in observations\n- Add shaped rewards (e.g., bonus for staying within \u00b12\u00b0C band)\n- Increase action magnitude granularity (\u00b11\u00b0C instead of \u00b13\u00b0C)\n- Reduce ambient noise or make it part of the observation\n- Longer episodes or curriculum learning with progressively harder scenarios\n</code></pre>"},{"location":"reinforcement-learning/3/#advanced-training-frontiers","title":"Advanced Training Frontiers","text":"<p>Congratulations! You've just built an MDP, wrapped it in a Gym, and solved it with a Deep RL algorithm. To wrap up the crash course, let's look at where the industry goes from here.</p> <p>What happens when environments are too complex for a standard PPO setup to learn from scratch?</p> <p>The Sparse Reward Problem</p> <p>Imagine training a robot to assemble a car. If the only reward is <code>+100</code> for a finished car and <code>0</code> for everything else, the robot will flail randomly for millions of years without ever accidentally building a car to get that first reward. How do we fix this?</p>"},{"location":"reinforcement-learning/3/#curriculum-learning","title":"Curriculum Learning","text":"<p>Instead of dropping the agent into the hardest version of the environment, we put it in \"school.\" We start with a ridiculously easy version of the environment (e.g., the car parts are mostly assembled). Once it masters that, we progressively make the environment harder until it solves the full problem.</p>"},{"location":"reinforcement-learning/3/#behavioral-cloning-imitation-learning","title":"Behavioral Cloning (Imitation Learning)","text":"<p>Why start with a blank brain? We can record a human expert playing the game (or controlling the thermostat). We then use supervised learning to train a network to copy the human's actions. Then, we hand that pre-trained network over to PPO to fine-tune it beyond human capabilities.</p>"},{"location":"reinforcement-learning/3/#multi-agent-reinforcement-learning-marl","title":"Multi-Agent Reinforcement Learning (MARL)","text":"<p>What if there are two thermostats controlled by two different agents?</p> <ul> <li>Cooperative: They must learn to work together without overriding each other's cooling.</li> <li>Competitive: They play against each other (e.g., Dota 2, Chess, Autonomous Racing). MARL is notoriously difficult because the environment becomes \"non-stationary\"\u2014from Agent A's perspective, the rules of the world seem to constantly change because Agent B is learning and changing its behavior simultaneously!</li> </ul>"},{"location":"reinforcement-learning/3/#rlhf-reinforcement-learning-from-human-feedback","title":"RLHF (Reinforcement Learning from Human Feedback)","text":"<p>This is the magic behind ChatGPT. How do you program a mathematically precise Reward Function for \"being polite\"? You can't. Instead, you have humans rate AI responses. You train a separate neural network (a Reward Model) to predict what a human would rate a response. Then, you use PPO to train the main AI, using the Reward Model as the environment's <code>step()</code> reward!</p>"},{"location":"reinforcement-learning/3/#crash-course-wrap-up","title":"Crash Course Wrap-Up","text":"<p>You now understand the pipeline:</p> <ol> <li>Formulate the real-world problem as a Markov Decision Process.</li> <li>Build a Gymnasium environment defining the Spaces and the step() physics.</li> <li>Select an algorithm like PPO from Stable-Baselines3.</li> <li>Tune hyperparameters and use advanced techniques if the problem is too complex.</li> </ol>"}]}